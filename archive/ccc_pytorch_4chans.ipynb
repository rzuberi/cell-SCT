{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook holds a pytorch CNN that classifies the cell crops into their cell cycle phase with 4 channels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ccc_nn_functions import str2array\n",
    "\n",
    "csv_file = r'C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\imported_CSV\\dataframe_821'\n",
    "df = pd.read_csv(csv_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[............................................................] 99/140034\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(ast.literal_eval(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[############################################################] 140034/140034\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#source:https://stackoverflow.com/questions/3160699/python-progress-bar\n",
    "def progressbar(it, prefix=\"\", size=60, out=sys.stdout): # Python3.3+\n",
    "    count = len(it)\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        print(\"{}[{}{}] {}/{}\".format(prefix, \"#\"*x, \".\"*(size-x), j, count),\n",
    "              end='\\r', file=out, flush=True)\n",
    "    show(0)\n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    print(\"\\n\", flush=True, file=out)\n",
    "\n",
    "def df_ignore_rows(df):\n",
    "    #indices_to_skip_img_wrong_shape = [i for i in range(len(df)) if str2array(df['pcna_crops'][i]).dtype is np.dtype('object')]  # skipping rows with shapes such as (7,)\n",
    "\n",
    "    indices_to_skip_img_wrong_shape = []\n",
    "    for i in progressbar(range(len(df))):\n",
    "        if str2array(df['pcna_crops'][i]).dtype is np.dtype('object'):\n",
    "            indices_to_skip_img_wrong_shape.append(i)\n",
    "\n",
    "\n",
    "    indices_to_skip_no_class = df[(df['G1_Phase'] == False) & (df['S_Phase'] == False) & (df['G2_M_Phase'] == False)].index\n",
    "\n",
    "    rows_to_ignore = np.concatenate((indices_to_skip_img_wrong_shape, indices_to_skip_no_class), axis=0)\n",
    "    df = df.drop(set(rows_to_ignore)).reset_index(drop=True) #dropping the rows to ignore\n",
    "\n",
    "    return df\n",
    "\n",
    "df = df_ignore_rows(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def get_crops(df):\n",
    "    dapi_crops = []\n",
    "    cyclina2_crops = []\n",
    "    edu_crops = []\n",
    "    pcna_crops = []\n",
    "    for i in progressbar(range(len(df)-110000)):\n",
    "        dapi_crops.append(str2array(df['dapi_crops'][i])*255)\n",
    "        cyclina2_crops.append(str2array(df['cyclina2_crops'][i])*255)\n",
    "        edu_crops.append(str2array(df['edu_crops'][i])*255)\n",
    "        pcna_crops.append(str2array(df['pcna_crops'][i])*255)\n",
    "    dapi_crops = np.array(dapi_crops)\n",
    "    cyclina2_crops = np.array(cyclina2_crops)\n",
    "    edu_crops = np.array(edu_crops)\n",
    "    pcna_crops = np.array(pcna_crops)\n",
    "\n",
    "    all_crops = np.dstack((dapi_crops,cyclina2_crops,edu_crops,pcna_crops)).squeeze()\n",
    "    return all_crops"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[############################################################] 16486/16486\r\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_21200\\2756014679.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dapi_crops = np.array(dapi_crops)\n",
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_21200\\2756014679.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cyclina2_crops = np.array(cyclina2_crops)\n",
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_21200\\2756014679.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  edu_crops = np.array(edu_crops)\n",
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_21200\\2756014679.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  pcna_crops = np.array(pcna_crops)\n"
     ]
    }
   ],
   "source": [
    "crops = get_crops(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1 31  1 37]\n",
      "  [ 1 31  1 38]\n",
      "  [ 1 30  1 38]\n",
      "  ...\n",
      "  [ 1 31  1 39]\n",
      "  [ 1 31  1 40]\n",
      "  [ 1 32  1 41]]\n",
      "\n",
      " [[ 1 30  1 39]\n",
      "  [ 1 31  1 38]\n",
      "  [ 1 30  0 39]\n",
      "  ...\n",
      "  [ 1 32  1 40]\n",
      "  [ 0 32  1 40]\n",
      "  [ 1 33  1 41]]\n",
      "\n",
      " [[ 1 31  1 38]\n",
      "  [ 1 31  0 38]\n",
      "  [ 0 30  1 39]\n",
      "  ...\n",
      "  [ 1 31  1 39]\n",
      "  [ 1 31  1 41]\n",
      "  [ 1 33  1 40]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1 31  1 38]\n",
      "  [ 1 30  1 38]\n",
      "  [ 1 31  1 40]\n",
      "  ...\n",
      "  [ 1 31  1 41]\n",
      "  [ 1 31  1 41]\n",
      "  [ 1 31  1 40]]\n",
      "\n",
      " [[ 1 30  1 38]\n",
      "  [ 1 31  1 38]\n",
      "  [ 1 30  1 39]\n",
      "  ...\n",
      "  [ 1 32  1 40]\n",
      "  [ 1 31  1 40]\n",
      "  [ 1 32  1 41]]\n",
      "\n",
      " [[ 1 29  1 38]\n",
      "  [ 1 29  1 38]\n",
      "  [ 1 31  1 40]\n",
      "  ...\n",
      "  [ 1 31  1 40]\n",
      "  [ 1 31  1 40]\n",
      "  [ 1 31  1 41]]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img_id = 1000\n",
    "arr_dapi = str2array(df['dapi_crops'][img_id])*255\n",
    "arr_cyclina2 = str2array(df['cyclina2_crops'][img_id])*255\n",
    "arr_edu = str2array(df['edu_crops'][img_id])*255\n",
    "arr_pcna = str2array(df['pcna_crops'][img_id])*255\n",
    "\n",
    "arr = np.dstack((arr_dapi,arr_cyclina2,arr_edu,arr_pcna))\n",
    "arr = arr.astype(np.uint8)\n",
    "\n",
    "print(arr)\n",
    "\n",
    "im = Image.fromarray(arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  0  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  ...\n",
      "  [ 1  1  2 ...  2  2  1]\n",
      "  [ 1  2  1 ...  2  2  1]\n",
      "  [ 1  1  2 ...  2  2  2]]\n",
      "\n",
      " [[15 13 13 ... 15 14 13]\n",
      "  [15 14 13 ... 17 16 16]\n",
      "  [14 15 15 ... 18 18 16]\n",
      "  ...\n",
      "  [19 20 20 ... 17 16 17]\n",
      "  [19 21 21 ... 17 17 17]\n",
      "  [21 20 22 ... 16 18 18]]\n",
      "\n",
      " [[ 0  0  1 ...  1  1  0]\n",
      "  [ 1  1  1 ...  0  1  0]\n",
      "  [ 0  0  0 ...  1  0  1]\n",
      "  ...\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]]\n",
      "\n",
      " [[20 21 22 ... 22 22 22]\n",
      "  [22 22 22 ... 21 23 21]\n",
      "  [23 21 24 ... 25 22 22]\n",
      "  ...\n",
      "  [26 25 25 ... 25 24 24]\n",
      "  [26 24 24 ... 24 26 25]\n",
      "  [24 26 28 ... 24 25 27]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 22), |u1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\PIL\\Image.py:2953\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   2952\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2953\u001B[0m     mode, rawmode \u001B[38;5;241m=\u001B[39m \u001B[43m_fromarray_typemap\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtypekey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   2954\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyError\u001B[0m: ((1, 1, 22), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [75]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     12\u001B[0m image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(image)\n\u001B[1;32m---> 14\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\PIL\\Image.py:2955\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   2953\u001B[0m         mode, rawmode \u001B[38;5;241m=\u001B[39m _fromarray_typemap[typekey]\n\u001B[0;32m   2954\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 2955\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot handle this data type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m typekey) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   2956\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2957\u001B[0m     rawmode \u001B[38;5;241m=\u001B[39m mode\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot handle this data type: (1, 1, 22), |u1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image = crops[0]*255\n",
    "\n",
    "image_to_np = []\n",
    "for arr in image:\n",
    "    image_to_np.append(np.array(arr))\n",
    "image = np.array(image_to_np)\n",
    "\n",
    "#image = np.expand_dims(image, axis=0)\n",
    "\n",
    "image = image.astype(np.uint8)\n",
    "print(image)\n",
    "image = Image.fromarray(image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_cell_labels(df):\n",
    "    g1_indices = df[(df['G1_Phase'] == True)].index\n",
    "    s_indices = df[(df['S_Phase'] == True)].index\n",
    "    g2_m_indices = df[(df['G2_M_Phase'] == True)].index\n",
    "\n",
    "    #make an array that is the length of all of these indices put into one, that is made of 0s\n",
    "    #replace the 0s accordingly by which phase index they correspond to\n",
    "\n",
    "    cell_labels = np.arange(len(g1_indices)+len(s_indices)+len(g2_m_indices))\n",
    "\n",
    "    np.put(cell_labels,g1_indices,np.zeros(len(g1_indices)))\n",
    "    np.put(cell_labels,s_indices,np.ones(len(s_indices)))\n",
    "    np.put(cell_labels,g2_m_indices,np.full(len(g2_m_indices),2))\n",
    "\n",
    "    return cell_labels\n",
    "\n",
    "cell_labels = get_cell_labels(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = torchvision.transforms.CenterCrop(self.output_size).forward(image)\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'label': torch.tensor(label)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, crops, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.crops = crops\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.crops[idx]\n",
    "\n",
    "\n",
    "        image_to_np = []\n",
    "        for arr in image:\n",
    "            image_to_np.append(np.array(arr))\n",
    "        image = np.array(image_to_np)\n",
    "\n",
    "        #image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        image = image.astype(np.uint8)\n",
    "        #print(image)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "\n",
    "transformed_dataset = CellDataset(crops=crops,\n",
    "                                        labels=cell_labels,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            CenterCrop(32),\n",
    "                                            ToTensor()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 22), |u1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\PIL\\Image.py:2953\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   2952\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2953\u001B[0m     mode, rawmode \u001B[38;5;241m=\u001B[39m \u001B[43m_fromarray_typemap\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtypekey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   2954\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyError\u001B[0m: ((1, 1, 22), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [87]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m img,label \u001B[38;5;241m=\u001B[39m \u001B[43mtransformed_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [85]\u001B[0m, in \u001B[0;36mCellDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     37\u001B[0m image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m#print(image)\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx]\n\u001B[0;32m     43\u001B[0m sample \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m: image, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m: label}\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\PIL\\Image.py:2955\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   2953\u001B[0m         mode, rawmode \u001B[38;5;241m=\u001B[39m _fromarray_typemap[typekey]\n\u001B[0;32m   2954\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 2955\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot handle this data type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m typekey) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   2956\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2957\u001B[0m     rawmode \u001B[38;5;241m=\u001B[39m mode\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot handle this data type: (1, 1, 22), |u1"
     ]
    }
   ],
   "source": [
    "img,label = transformed_dataset.__getitem__(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}