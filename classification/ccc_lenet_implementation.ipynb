{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from importlib import reload\n",
    "import ccc_nn_functions\n",
    "from ccc_nn_functions import *\n",
    "ccc_nn_functions = reload(ccc_nn_functions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU is enabled:',torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "#Train a model\n",
    "\n",
    "# Data path\n",
    "df_821_short_path = r'C:\\Users\\rz200\\Documents\\Development\\cell-SCT\\classification\\imported_CSV\\dataframe_821_short'\n",
    "df_821_path = r'C:\\Users\\rz200\\Documents\\Development\\cell-SCT\\classification\\imported_CSV\\dataframe_821'\n",
    "data_path = df_821_path #path to your data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell cycle classification CNN on PCNA channel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:96: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(ast.literal_eval(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 91069 | Validation example: 10119 | Testing examples: 25298\n",
      "The model has 61,111 trainable parameters\n",
      "Torch Cuda is available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4a1dd6259e54fd1b95d2ced7ba9faef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/1423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a6f1d286a694ad09e4dc7ed6f85d6ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59dca4c09540491db940337fea1cf672"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 54s | Train Loss: 0.826 | Train Acc: 62.98% | Val. Loss: 0.770 |  Val. Acc: 64.94%\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/1423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fec8dbb4dcb46f2b93e0edacd449395"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e425ba3ced044fb681b3480054f99f89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 2m 52s | Train Loss: 0.760 | Train Acc: 65.84% | Val. Loss: 0.754 |  Val. Acc: 65.97%\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/1423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08a11fde1e11492b8ea8096139375d8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6abf28c28d1462fa322b68497f3e78d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 2m 55s | Train Loss: 0.744 | Train Acc: 66.50% | Val. Loss: 0.739 |  Val. Acc: 66.89%\n"
     ]
    }
   ],
   "source": [
    "# Path to save model\n",
    "path_to_save = r'C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\saved_models\\test_model.pt' #path to which you want to save your model\n",
    "\n",
    "# Training the model\n",
    "iterators = get_iterators(data_path) #get the iterators: data train-test-val formatted\n",
    "num_epochs = 3\n",
    "train_ccc_model(num_epochs,path_to_save,iterators) #train the model\n",
    "model = load_model(path_to_save) #load your model you just trained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/396 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb223c02d3bf4a4eae64982002187db5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.743 | Test Acc: 66.19%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluate your model on your test data\n",
    "test_iterator = iterators[2]\n",
    "test_loss, test_acc = evaluate(model, test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "#Confusion matrix\n",
    "images, labels, probs = get_predictions(model, test_iterator)\n",
    "pred_labels = torch.argmax(probs, 1)\n",
    "plot_confusion_matrix(labels, pred_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell cycle classification CNN on DAPI channel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(df_821_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:98: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(ast.literal_eval(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 91069 | Validation example: 10119 | Testing examples: 25298\n"
     ]
    }
   ],
   "source": [
    "# Path to save model\n",
    "path_to_save = r'C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\saved_models\\test_model_DAPI.pt' #path to which you want to save your model\n",
    "\n",
    "# Training the model\n",
    "iterators = get_iterators(data_path,'dapi_crops') #get the iterators: data train-test-val formatted\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 61,111 trainable parameters\n",
      "Torch Cuda is available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1726e45f43d94c21bfb7ce9a72d246c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/1423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d5b28af175b403b9d23941cae1214d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdd8165d5418436e8f40b91b7ab29707"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 13s | Train Loss: 0.767 | Train Acc: 61.78% | Val. Loss: 0.724 |  Val. Acc: 63.48%\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/1423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e31a9481a1843368925e611f1b63d50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec316b76fe6e487b8d7b33ac70d6cb51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 3m 3s | Train Loss: 0.706 | Train Acc: 65.26% | Val. Loss: 0.685 |  Val. Acc: 66.62%\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/1423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "970cb4cd24c348369abd0864a4e002a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d32a49f0bb44df5a6a0d8793f23ca9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 3m 2s | Train Loss: 0.691 | Train Acc: 66.16% | Val. Loss: 0.689 |  Val. Acc: 66.68%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "train_ccc_model(num_epochs,path_to_save,iterators) #train the model\n",
    "model = load_model(path_to_save) #load your model you just trained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a CNN on 4 channels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import ccc_nn_functions\n",
    "from ccc_nn_functions import *\n",
    "ccc_nn_functions = reload(ccc_nn_functions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df_821_path = r'C:\\Users\\rz200\\Documents\\Development\\cell-SCT\\classification\\imported_CSV\\dataframe_821'\n",
    "data_path = df_821_path #path to your data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(ast.literal_eval(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 91069 | Validation example: 10119 | Testing examples: 25298\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_ccc_model() missing 1 required positional argument: 'channel_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m iterators \u001B[38;5;241m=\u001B[39m get_iterators(data_path,channel_names) \u001B[38;5;66;03m#get the iterators: data train-test-val formatted\u001B[39;00m\n\u001B[0;32m      7\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[1;32m----> 8\u001B[0m \u001B[43mtrain_ccc_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mpath_to_save\u001B[49m\u001B[43m,\u001B[49m\u001B[43miterators\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#train the model\u001B[39;00m\n\u001B[0;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m load_model(path_to_save)\n",
      "\u001B[1;31mTypeError\u001B[0m: train_ccc_model() missing 1 required positional argument: 'channel_names'"
     ]
    }
   ],
   "source": [
    "# Path to save model\n",
    "\n",
    "\n",
    "# Training the model\n",
    "channel_names = ['dapi_crops','edu_crops','cyclina2_crops','pcna_crops']\n",
    "iterators = get_iterators(data_path,channel_names) #get the iterators: data train-test-val formatted\n",
    "num_epochs = 3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 61,561 trainable parameters\n",
      "Torch Cuda is available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f0577792b9243908f388b16dabb9a05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/1423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "454fdb9e90b04ee8b48bcc5cb9bbc51b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 0]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m path_to_save \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mrz200\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDocuments\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdevelopment\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcell-SCT\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mclassification\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msaved_models\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest_model_all_channels.pt\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;66;03m#path to which you want to save your model\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrain_ccc_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mpath_to_save\u001B[49m\u001B[43m,\u001B[49m\u001B[43miterators\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannel_names\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#train the model\u001B[39;00m\n\u001B[0;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m load_model(path_to_save)\n",
      "File \u001B[1;32m~\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:278\u001B[0m, in \u001B[0;36mtrain_ccc_model\u001B[1;34m(epochs, path_to_save_model, iterators, channel_names)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m trange(EPOCHS, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpochs\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    277\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[1;32m--> 278\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iterator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    279\u001B[0m     valid_loss, valid_acc \u001B[38;5;241m=\u001B[39m evaluate(model, valid_iterator)\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m valid_loss \u001B[38;5;241m<\u001B[39m best_valid_loss:\n",
      "File \u001B[1;32m~\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:175\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, iterator, optimizer, criterion, device)\u001B[0m\n\u001B[0;32m    171\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    173\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 175\u001B[0m y_pred, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(y_pred, y)\n\u001B[0;32m    179\u001B[0m acc \u001B[38;5;241m=\u001B[39m calculate_accuracy(y_pred, y)\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:127\u001B[0m, in \u001B[0;36mLeNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;66;03m# x = [batch size, 1, 28, 28]\u001B[39;00m\n\u001B[1;32m--> 127\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;66;03m# x = [batch size, 6, 24, 24]\u001B[39;00m\n\u001B[0;32m    129\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmax_pool2d(x, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 447\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    441\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    442\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 0]"
     ]
    }
   ],
   "source": [
    "path_to_save = r'C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\saved_models\\test_model_all_channels.pt' #path to which you want to save your model\n",
    "\n",
    "train_ccc_model(num_epochs,path_to_save,iterators, channel_names) #train the model\n",
    "model = load_model(path_to_save) #load your model you just trained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_4544\\161000061.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_shuffled['ground_truth'] = cycle_phase_encodings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:98: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(ast.literal_eval(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001/80000\n",
      "2001/80000\n",
      "3001/80000\n",
      "4001/80000\n",
      "5001/80000\n",
      "6001/80000\n",
      "7001/80000\n",
      "8001/80000\n",
      "9001/80000\n",
      "10001/80000\n",
      "11001/80000\n",
      "12001/80000\n",
      "13001/80000\n",
      "14001/80000\n",
      "15001/80000\n",
      "16001/80000\n",
      "17001/80000\n",
      "18001/80000\n",
      "19001/80000\n",
      "20001/80000\n",
      "21001/80000\n",
      "22001/80000\n",
      "23001/80000\n",
      "24001/80000\n",
      "25001/80000\n",
      "26001/80000\n",
      "27001/80000\n",
      "28001/80000\n",
      "29001/80000\n",
      "30001/80000\n",
      "31001/80000\n",
      "32001/80000\n",
      "33001/80000\n",
      "34001/80000\n",
      "35001/80000\n",
      "36001/80000\n",
      "37001/80000\n",
      "38001/80000\n",
      "39001/80000\n",
      "40001/80000\n",
      "41001/80000\n",
      "42001/80000\n",
      "43001/80000\n",
      "44001/80000\n",
      "45001/80000\n",
      "46001/80000\n",
      "47001/80000\n",
      "48001/80000\n",
      "49001/80000\n",
      "50001/80000\n",
      "51001/80000\n",
      "52001/80000\n",
      "53001/80000\n",
      "54001/80000\n",
      "55001/80000\n",
      "56001/80000\n",
      "57001/80000\n",
      "58001/80000\n",
      "59001/80000\n",
      "60001/80000\n",
      "61001/80000\n",
      "62001/80000\n",
      "63001/80000\n",
      "64001/80000\n",
      "65001/80000\n",
      "66001/80000\n",
      "67001/80000\n",
      "68001/80000\n",
      "69001/80000\n",
      "70001/80000\n",
      "71001/80000\n",
      "72001/80000\n",
      "73001/80000\n",
      "74001/80000\n",
      "75001/80000\n",
      "76001/80000\n",
      "77001/80000\n",
      "78001/80000\n",
      "79001/80000\n",
      "160001/140034\n",
      "161001/140034\n",
      "162001/140034\n",
      "163001/140034\n",
      "164001/140034\n",
      "165001/140034\n",
      "166001/140034\n",
      "167001/140034\n",
      "168001/140034\n",
      "169001/140034\n",
      "170001/140034\n",
      "171001/140034\n",
      "172001/140034\n",
      "173001/140034\n",
      "174001/140034\n",
      "175001/140034\n",
      "176001/140034\n",
      "177001/140034\n",
      "178001/140034\n",
      "179001/140034\n",
      "180001/140034\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "100000",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\pandas\\core\\indexes\\range.py:385\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_range\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[1;31mValueError\u001B[0m: 100000 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [70]\u001B[0m, in \u001B[0;36m<cell line: 114>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    110\u001B[0m             im\u001B[38;5;241m.\u001B[39msave(path)\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;66;03m#no need to return anything\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m \u001B[43msort_training_and_testing_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [70]\u001B[0m, in \u001B[0;36msort_training_and_testing_data\u001B[1;34m(df, test_size)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(last_train_row,\u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mindex)):\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mstr\u001B[39m(i\u001B[38;5;241m+\u001B[39mlast_train_row\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mindex)))\n\u001B[1;32m---> 86\u001B[0m     arr \u001B[38;5;241m=\u001B[39m str2array(\u001B[43mdf_shuffled\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpcna_crops\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m     88\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\pandas\\core\\series.py:958\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    955\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[0;32m    957\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[1;32m--> 958\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[0;32m    961\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[0;32m    962\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    963\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\pandas\\core\\series.py:1069\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[1;34m(self, label, takeable)\u001B[0m\n\u001B[0;32m   1066\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[0;32m   1068\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[1;32m-> 1069\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_get_values_for_loc(\u001B[38;5;28mself\u001B[39m, loc, label)\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\pandas\\core\\indexes\\range.py:387\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m    385\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_range\u001B[38;5;241m.\u001B[39mindex(new_key)\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m--> 387\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[0;32m    389\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 100000"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "def sort_training_and_testing_data(df,test_size=0.2):\n",
    "    shutil.rmtree('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\data')\n",
    "\n",
    "    #create a new dataframe where we shuffle the rows\n",
    "    df_shuffled = np.copy(df)\n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True) #we could keep the index to keep for the cell names so its consistent with the first dataframe, would hepl later on\n",
    "    df_shuffled = df[:100000]\n",
    "\n",
    "    #create a new column in that dataframe called 'ground_truth'\n",
    "    #for each row, if there is one True phase, that's the ground_truth\n",
    "    #if there are no True phase or more than two, the ground_truth is 'unclassified' for now\n",
    "    cycle_phase_encodings = []\n",
    "    for i in range(len(df_shuffled['S_Phase'])):\n",
    "        encoding_arr = []\n",
    "        if df_shuffled['G1_Phase'][i]: encoding_arr.append('g1_phase')\n",
    "        #else: encoding_arr.append(0)\n",
    "\n",
    "        if df_shuffled['S_Phase'][i]: encoding_arr.append('s_phase')\n",
    "        #else: encoding_arr.append(0)\n",
    "\n",
    "        if df_shuffled['G2_M_Phase'][i]: encoding_arr.append('g2_m_phase')\n",
    "        #else: encoding_arr.append(0)\n",
    "\n",
    "        if len(encoding_arr) != 1: encoding_arr = ['unclassified']\n",
    "\n",
    "        cycle_phase_encodings.append(encoding_arr)\n",
    "\n",
    "    df_shuffled['ground_truth'] = cycle_phase_encodings\n",
    "\n",
    "    #create a data directory\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data')\n",
    "\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\training_data')\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\training_data\\\\g1_phase')\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\training_data\\\\s_phase')\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\training_data\\\\g2_m_phase')\n",
    "\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\testing_data')\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\testing_data\\\\g1_phase')\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\testing_data\\\\s_phase')\n",
    "    os.mkdir('C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\testing_data\\\\g2_m_phase')\n",
    "\n",
    "    #take the first 1-test_size (0.8) and put them in the train folders\n",
    "    #create a training_data directory inside the data directory\n",
    "    #create an s_phase, g1_phase and g2_m_phase directories in there\n",
    "    #for each row in the first 0.8 of the dataframe, put the EdU numpy arrays in the corresponding folders\n",
    "    last_train_row = round(len(df_shuffled.index)*(1-test_size))\n",
    "    for i in range(last_train_row):\n",
    "        if i % 1000 == 0: print(str(i+1) + '/' + str(last_train_row))\n",
    "        name = 'cell_' + str(i)\n",
    "\n",
    "        arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "        if arr.dtype is np.dtype('object'):\n",
    "            continue\n",
    "\n",
    "\n",
    "        if df_shuffled['ground_truth'][i] == ['g1_phase']:\n",
    "            path = 'C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\training_data\\\\g1_phase\\\\' + name + '.png'\n",
    "            arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "            im = Image.fromarray(arr)\n",
    "            im = im.convert(\"L\")\n",
    "            im.save(path)\n",
    "\n",
    "        elif df_shuffled['ground_truth'][i] == ['s_phase']:\n",
    "            path = 'C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\training_data\\\\s_phase\\\\' + name + '.png'\n",
    "            arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "            im = Image.fromarray(arr)\n",
    "            im = im.convert(\"L\")\n",
    "            im.save(path)\n",
    "\n",
    "        elif df_shuffled['ground_truth'][i] == ['g2_m_phase']:\n",
    "            path = 'C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\training_data\\\\g2_m_phase\\\\' + name + '.png'\n",
    "            arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "            im = Image.fromarray(arr)\n",
    "            im = im.convert(\"L\")\n",
    "            im.save(path)\n",
    "\n",
    "    #do the same for the testing data\n",
    "    for i in range(last_train_row,len(df.index)):\n",
    "        if i % 1000 == 0: print(str(i+last_train_row+1) + '/' + str(len(df.index)))\n",
    "\n",
    "        arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "        if arr.dtype is np.dtype('object'):\n",
    "            continue\n",
    "\n",
    "        name = 'cell_' + str(i)\n",
    "        if df_shuffled['ground_truth'][i] == ['g1_phase']:\n",
    "            path = 'C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\testing_data\\\\g1_phase\\\\' + name + '.png'\n",
    "            arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "            im = Image.fromarray(arr)\n",
    "            im = im.convert(\"L\")\n",
    "            im.save(path)\n",
    "\n",
    "        elif df_shuffled['ground_truth'][i] == ['s_phase']:\n",
    "            path = 'C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\testing_data\\\\s_phase\\\\' + name + '.png'\n",
    "            arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "            im = Image.fromarray(arr)\n",
    "            im = im.convert(\"L\")\n",
    "            im.save(path)\n",
    "\n",
    "        elif df_shuffled['ground_truth'][i] == ['g2_m_phase']:\n",
    "            path = 'C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\cell-SCT\\\\classification\\\\data\\\\testing_data\\\\g2_m_phase\\\\' + name + '.png'\n",
    "            arr = str2array(df_shuffled['pcna_crops'][i])\n",
    "            im = Image.fromarray(arr)\n",
    "            im = im.convert(\"L\")\n",
    "            im.save(path)\n",
    "\n",
    "    #no need to return anything\n",
    "\n",
    "sort_training_and_testing_data(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = r'C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\data'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.11.0+cu113\n",
      "Torchvision Version:  0.12.0+cu113\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Done!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU is enabled:',torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.0881 Acc: 0.4311\n",
      "val Loss: 1.0413 Acc: 0.3670\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.0962 Acc: 0.4352\n",
      "val Loss: 0.9863 Acc: 0.5008\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 1.0879 Acc: 0.4323\n",
      "val Loss: 1.0376 Acc: 0.3669\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.0951 Acc: 0.4318\n",
      "val Loss: 1.0108 Acc: 0.5008\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.0949 Acc: 0.4326\n",
      "val Loss: 1.0932 Acc: 0.5004\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Train and evaluate\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m model_ft, hist \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_inception\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minception\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001B[0m\n\u001B[0;32m     58\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss1 \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.4\u001B[39m\u001B[38;5;241m*\u001B[39mloss2\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 60\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     63\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torchvision\\models\\resnet.py:283\u001B[0m, in \u001B[0;36mResNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torchvision\\models\\resnet.py:271\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    268\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[0;32m    269\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxpool(x)\n\u001B[1;32m--> 271\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    272\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[0;32m    273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torchvision\\models\\resnet.py:90\u001B[0m, in \u001B[0;36mBasicBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m     88\u001B[0m     identity \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m---> 90\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(out)\n\u001B[0;32m     92\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(out)\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 447\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\celldev2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    441\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    442\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No such operator profiler::_record_function_enter_new",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#We can try a model with pre-trained weights\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#source: https://pytorch.org/vision/stable/models.html\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m read_image\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m resnet50, ResNet50_Weights\n\u001B[0;32m      7\u001B[0m img \u001B[38;5;241m=\u001B[39m read_image(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpcna_crops\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torchvision\\__init__.py:7\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m io\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torchvision\\models\\__init__.py:2\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malexnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconvnext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdensenet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mefficientnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torchvision\\models\\convnext.py:8\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn, Tensor\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Permute\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstochastic_depth\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StochasticDepth\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_presets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageClassification\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torchvision\\ops\\__init__.py:18\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeform_conv\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deform_conv2d, DeformConv2d\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdiou_loss\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distance_box_iou_loss\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdrop_block\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drop_block2d, DropBlock2d, drop_block3d, DropBlock3d\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_pyramid_network\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FeaturePyramidNetwork\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfocal_loss\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sigmoid_focal_loss\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torchvision\\ops\\drop_block.py:2\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn, Tensor\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\fx\\__init__.py:83\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mFX is a toolkit for developers to use to transform ``nn.Module``\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03minstances. FX consists of three main components: a **symbolic tracer,**\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124;03mrepository.\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph_module\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GraphModule\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_symbolic_trace\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m symbolic_trace, Tracer, wrap, PH, ProxyableClassMeta\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Graph, CodeGen\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\fx\\graph_module.py:8\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlinecache\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Type, Dict, List, Any, Union, Optional, Set\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Graph, _PyTreeCodeGen, _is_from_torch, _custom_builtins, PythonCode\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compatibility\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compatibility\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Importer, sys_importer\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\fx\\graph.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnode\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Node, Argument, Target, map_arg, _type_repr, _get_qualified_name\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_pytree\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpytree\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pytree \u001B[38;5;28;01mas\u001B[39;00m fx_pytree\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\fx\\node.py:31\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m Target \u001B[38;5;241m=\u001B[39m Union[Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, Any], \u001B[38;5;28mstr\u001B[39m]\n\u001B[0;32m     19\u001B[0m Argument \u001B[38;5;241m=\u001B[39m Optional[Union[\n\u001B[0;32m     20\u001B[0m     Tuple[Any, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m],  \u001B[38;5;66;03m# actually Argument, but mypy can't represent recursive types\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     List[Any],  \u001B[38;5;66;03m# actually Argument\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     BaseArgumentTypes\n\u001B[0;32m     26\u001B[0m ]]\n\u001B[0;32m     28\u001B[0m _side_effectful_functions: Set[Callable] \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     29\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_assert,\n\u001B[0;32m     30\u001B[0m     torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39m_record_function_enter,\n\u001B[1;32m---> 31\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprofiler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_record_function_enter_new\u001B[49m,\n\u001B[0;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39m_record_function_exit}\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# this is fixed on master, WAR for 1.5\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_find_module_of_method\u001B[39m(orig_method: Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\_ops.py:167\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(self, op_name)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_OpNamespace\u001B[39;00m(types\u001B[38;5;241m.\u001B[39mModuleType):\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;124;03m    An op namespace to dynamically bind Operators into Python.\u001B[39;00m\n\u001B[1;32m--> 167\u001B[0m \n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03m    Say a user has created a custom Operator called \"my_namespace::my_op\". To\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03m    call this op, the user will write torch.ops.my_namespace.my_op(...).\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m    At startup, this operation will not yet be bound into Python. Instead, the\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;124;03m    following sequence of magic tricks will occur:\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;124;03m    1. `torch.ops.my_namespace` will invoke the `__getattr__` magic method\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;124;03m       on the `torch.ops` object, which will create a new `_OpNamespace`\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;124;03m       object called `my_namespace` and set it as an attribute on the `ops`\u001B[39;00m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;124;03m       object.\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;124;03m    2. `torch.ops.my_namespace.my_op` will then invoke `__getattr__` on\u001B[39;00m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m       the `my_namespace` object, which will retrieve the operation via\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;124;03m       `torch.get_operation`, a function bound from C++, and then in a similar\u001B[39;00m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;124;03m       fashion bind this new object onto the `my_namespace` object.\u001B[39;00m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;124;03m    3. `torch.ops.my_namespace.my_op(...)` then calls this new operation\u001B[39;00m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;124;03m        and subsequent accesses will incur no further lookup (the namespace and\u001B[39;00m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;124;03m        operation will already exist).\u001B[39;00m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[0;32m    185\u001B[0m         \u001B[38;5;28msuper\u001B[39m(_OpNamespace, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtorch.ops.\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m name)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: No such operator profiler::_record_function_enter_new"
     ]
    }
   ],
   "source": [
    "#We can try a model with pre-trained weights\n",
    "#source: https://pytorch.org/vision/stable/models.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Now we should make all of this a pipeline where you just need to give the CSV file. Done\n",
    "#Also we should have a prediction function. Already done\n",
    "#Later on we need to modify this to be a mask to add on the segmented cells\n",
    "    #The input to the network has to be an image made just from the segmentation mask of the cell, and the rest is padded to be squared, so no background\n",
    "    #The cells need to be kept with an index for the segmentation mask of the cell to be brought back as labelled in the end"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#I should try making a better segmentation model with CellPose2, then segmenting out the cells from both plates into two CSVs that we then put together, and then training this classification model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Try the same model but with the CyclinA2 crops"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make a model to try and guess the DAPI, CyclinA2 and EdU values just from the PCNA values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Next steps\n",
    " #Add a 'column name' input to choose which channel crops we want to train the model on. Doing it.\n",
    " #Add a function to just give a cell image and it gets the segmentation and it gets the predictions of all of its cells as a mask, it should keep the coordinates of the cell crops it makes\n",
    " #Change the model to only take the segmentation mask to make the prediction, not the crops\n",
    " #Make a new notebook that presents all of these functions together as a single pipeline\n",
    " #Make a GUI that presents the cell masks and we can hover over a cell to showcase it and it would give its cell phase\n",
    " #Look into Grad-CAM to get heat maps of regions with a greater weight in CNN classification\n",
    " #Paper consolidation of the research on training a model for cell cycle classification, the data they had, the model they used and their results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}