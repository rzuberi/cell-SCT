{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I want to see if from the PCNA values, a model can output the DAPI, Cyclin A2 and EdU values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: numpy in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from torchvision) (1.11.0+cu113)\n",
      "Requirement already satisfied: requests in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from requests->torchvision) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages (from requests->torchvision) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     C:\\ProgramData\\Anaconda3\n",
      "celldev               *  C:\\Users\\rz200\\.conda\\envs\\celldev\n",
      "datadev                  C:\\Users\\rz200\\.conda\\envs\\datadev\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#I think we would want to go from the PCNA crop to guess the average DAPI value\n",
    "\n",
    "#Let's get the PCNA numpys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ccc_nn_functions import str2array\n",
    "\n",
    "csv_file = r'C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\imported_CSV\\dataframe_822'\n",
    "df = pd.read_csv(csv_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\Documents\\development\\cell-SCT\\classification\\ccc_nn_functions.py:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(ast.literal_eval(s))\n"
     ]
    }
   ],
   "source": [
    "def df_ignore_rows(df):\n",
    "    indices_to_skip_img_wrong_shape = [i for i in range(len(df)) if str2array(df['pcna_crops'][i]).dtype is np.dtype('object')]  # skipping rows with shapes such as (7,)\n",
    "    indices_to_skip_no_class = df[(df['G1_Phase'] == False) & (df['S_Phase'] == False) & (df['G2_M_Phase'] == False)].index\n",
    "\n",
    "    rows_to_ignore = np.concatenate((indices_to_skip_img_wrong_shape, indices_to_skip_no_class), axis=0)\n",
    "    df = df.drop(set(rows_to_ignore)).reset_index(drop=True) #dropping the rows to ignore\n",
    "\n",
    "    return df\n",
    "\n",
    "df = df_ignore_rows(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_crops_flat_pad(df,column_name):\n",
    "    pcna_crops = []\n",
    "    for i in range(len(df)): pcna_crops.append(str2array(df[column_name][i]))\n",
    "    #We'll want to flatten all of these arrays\n",
    "    pcna_crops_flat = []\n",
    "    for i in range(len(pcna_crops)):\n",
    "        pcna_crops_flat.append(pcna_crops[i].flatten())\n",
    "    #Then we want to get the longest one\n",
    "    max_shape = max([flat_crop.shape[0] for flat_crop in pcna_crops_flat])\n",
    "    #Then we want to add 0s at the end of everyone that isn't as long as the longest one\n",
    "    pcna_crops_flat_pad = []\n",
    "    for i in range(len(pcna_crops_flat)):\n",
    "        A = pcna_crops_flat[i]\n",
    "        pad_size = max_shape - A.shape[0]\n",
    "        new_arr = np.pad(A, (0, pad_size), 'constant')\n",
    "        pcna_crops_flat_pad.append(new_arr)\n",
    "    pcna_crops_flat_pad = np.array(pcna_crops_flat_pad)\n",
    "\n",
    "    return pcna_crops_flat_pad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.98933053016663\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "pcna_crops_flat_pad = get_crops_flat_pad(df,'pcna_crops')\n",
    "print(time.time()-start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79386, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(pcna_crops_flat_pad.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pcna_crops = []\n",
    "for i in range(len(df)): pcna_crops.append(str2array(df['pcna_crops'][i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79386,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_24132\\4224422109.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  pcna_crops = np.array(pcna_crops)\n"
     ]
    }
   ],
   "source": [
    "pcna_crops = np.array(pcna_crops)\n",
    "print(pcna_crops.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m imread, imshow\n\u001B[1;32m----> 2\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mxy_to_binary2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpcna_crops\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m img \u001B[38;5;241m=\u001B[39m imread(image)\n\u001B[0;32m      4\u001B[0m imshow(img)\n",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36mxy_to_binary2d\u001B[1;34m(ts)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ts\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint32\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOnly integer input is supported.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m xmax,ymax \u001B[38;5;241m=\u001B[39m ts\u001B[38;5;241m.\u001B[39mmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      7\u001B[0m __,ymin \u001B[38;5;241m=\u001B[39m ts\u001B[38;5;241m.\u001B[39mmin(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ymin \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from skimage.io import imread, imshow\n",
    "image = xy_to_binary2d(pcna_crops[0].astype(np.int32))\n",
    "img = imread(image)\n",
    "imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mexpand_dims(pcna_crops[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mint32),axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      2\u001B[0m image\u001B[38;5;241m.\u001B[39mndim\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "impo\n",
    "image = np.expand_dims(pcna_crops[0].astype(np.int32),axis=0)\n",
    "image.ndim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, channel_axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_3920\\2860752918.py:2: FutureWarning: `multichannel` is a deprecated argument name for `hog`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  fd, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [35]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hog\n\u001B[1;32m----> 2\u001B[0m fd, hog_image \u001B[38;5;241m=\u001B[39m \u001B[43mhog\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morientations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m9\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpixels_per_cell\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcells_per_block\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmultichannel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\skimage\\_shared\\utils.py:394\u001B[0m, in \u001B[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    391\u001B[0m channel_axis \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchannel_axis\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    393\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m channel_axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 394\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001B[39;00m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001B[39;00m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(channel_axis):\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\skimage\\_shared\\utils.py:348\u001B[0m, in \u001B[0;36mdeprecate_multichannel_kwarg.__call__.<locals>.fixed_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    345\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchannel_axis\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m convert[kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmultichannel\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m    347\u001B[0m \u001B[38;5;66;03m# Call the function with the fixed arguments\u001B[39;00m\n\u001B[1;32m--> 348\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\skimage\\feature\\_hog.py:284\u001B[0m, in \u001B[0;36mhog\u001B[1;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, multichannel, channel_axis)\u001B[0m\n\u001B[0;32m    282\u001B[0m n_blocks_row \u001B[38;5;241m=\u001B[39m (n_cells_row \u001B[38;5;241m-\u001B[39m b_row) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    283\u001B[0m n_blocks_col \u001B[38;5;241m=\u001B[39m (n_cells_col \u001B[38;5;241m-\u001B[39m b_col) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 284\u001B[0m normalized_blocks \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_blocks_row\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_blocks_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb_row\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morientations\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfloat_dtype\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_blocks_row):\n\u001B[0;32m    290\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_blocks_col):\n",
      "\u001B[1;31mValueError\u001B[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "fd, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, multichannel=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "dapi_crops_flat_pad = get_crops_flat_pad(df,'dapi_crops')\n",
    "cyclina2_crops_flat_pad = get_crops_flat_pad(df,'cyclina2_crops')\n",
    "edu_crops_flat_pad = get_crops_flat_pad(df,'edu_crops')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_label_values(df):\n",
    "    dapi_values = []\n",
    "    cyclina2_values = []\n",
    "    edu_values = []\n",
    "    dna_values = []\n",
    "    pcna_values = []\n",
    "    for i in range(len(df)):\n",
    "        dapi_values.append(df['dapi_values'][i])\n",
    "        cyclina2_values.append(df['cyclina2_values'][i])\n",
    "        edu_values.append(df['edu_values'][i])\n",
    "        dna_values.append(df['DNA_content'][i])\n",
    "        pcna_values.append(df['pcna_values'][i])\n",
    "    dapi_values = np.array(dapi_values)\n",
    "    cyclina2_values = np.array(cyclina2_values)\n",
    "    edu_values = np.array(edu_values)\n",
    "    dna_values = np.array(dna_values)\n",
    "    pcna_values = np.array(pcna_values)\n",
    "\n",
    "    return dapi_values,cyclina2_values,edu_values,dna_values, pcna_values\n",
    "\n",
    "dapi_values, cyclina2_values, edu_values, dna_values, pcna_values = get_label_values(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statistics import mean\n",
    "\n",
    "def off_score(model,X,y):\n",
    "    predictions = model.predict(X)\n",
    "    result = abs(predictions - y)\n",
    "    return mean(result)\n",
    "\n",
    "def off_perc(model,X,y):\n",
    "    predictions = model.predict(X)\n",
    "    result = abs(predictions - y)\n",
    "    percentage_off = []\n",
    "    for i in range(len(predictions)):\n",
    "        percentage_off.append(result[i]*100/y[i])\n",
    "    mean_perc = mean(percentage_off)\n",
    "    return 100 - mean_perc\n",
    "\n",
    "def get_reg_model(images,values):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, values, test_size=0.4, random_state=42)\n",
    "    reg = LinearRegression().fit(X_train,y_train)\n",
    "    print('Off score:',off_score(reg,X_test,y_test),'|','Accuracy:',str(round(off_perc(reg,X_test,y_test),2)) + '%')\n",
    "    return reg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dapi_mod = get_reg_model(pcna_crops_flat_pad,dapi_values)\n",
    "cyclina2_mod = get_reg_model(pcna_crops_flat_pad,cyclina2_values)\n",
    "edu_mod = get_reg_model(pcna_crops_flat_pad,edu_values)\n",
    "dna_mod = get_reg_model(pcna_crops_flat_pad,dna_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Need to make a column with the label of the cells\n",
    "\n",
    "def get_cell_labels(df):\n",
    "    g1_indices = df[(df['G1_Phase'] == True)].index\n",
    "    s_indices = df[(df['S_Phase'] == True)].index\n",
    "    g2_m_indices = df[(df['G2_M_Phase'] == True)].index\n",
    "\n",
    "    #make an array that is the length of all of these indices put into one, that is made of 0s\n",
    "    #replace the 0s accordingly by which phase index they correspond to\n",
    "\n",
    "    cell_labels = np.arange(len(g1_indices)+len(s_indices)+len(g2_m_indices))\n",
    "\n",
    "    np.put(cell_labels,g1_indices,np.zeros(len(g1_indices)))\n",
    "    np.put(cell_labels,s_indices,np.ones(len(s_indices)))\n",
    "    np.put(cell_labels,g2_m_indices,np.full(len(g2_m_indices),2))\n",
    "\n",
    "    return cell_labels\n",
    "\n",
    "cell_labels = get_cell_labels(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126486\n",
      "126486\n"
     ]
    }
   ],
   "source": [
    "print(len(pcna_crops_flat_pad))\n",
    "print(len(cell_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Off score: 0.5087350430985463 | Accuracy: -inf%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\AppData\\Local\\Temp\\ipykernel_19392\\1862398129.py:15: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  percentage_off.append(result[i]*100/y[i])\n"
     ]
    }
   ],
   "source": [
    "#Flat PCNA images to label\n",
    "pcna_flat_mod = get_reg_model(pcna_crops_flat_pad,cell_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.21879632374741\n"
     ]
    }
   ],
   "source": [
    "#Flat PCNA images to just predict g1\n",
    "\n",
    "#Here we will change cell_labels for all of the 0s to be 1s and all of the 1s and 2s to be 0s\n",
    "#That will mean that not G1 is 0, and G1 is 1\n",
    "g1_labels = [] #we will store in there the 0s for 1s and 2s in cell_labels and 1s for the 0s that correspond to G1\n",
    "for label in cell_labels:\n",
    "    if label == 0: #thats a G1\n",
    "        g1_labels.append(1)\n",
    "    else: g1_labels.append(0)\n",
    "\n",
    "#Now lets train a model on this\n",
    "images = pcna_crops_flat_pad\n",
    "values = g1_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, values, test_size=0.4, random_state=42)\n",
    "reg = LinearRegression().fit(X_train,y_train)\n",
    "predictions = np.round(reg.predict(X_test)).astype(int) #rounding the values to 0 or 1\n",
    "correct = np.count_nonzero(y_test == predictions)\n",
    "print('Accuracy:',correct*100/len(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.75798003755312\n"
     ]
    }
   ],
   "source": [
    "#Flat PCNA images to just predict s\n",
    "\n",
    "s_labels = []\n",
    "for label in cell_labels:\n",
    "    if label == 1: #that an S\n",
    "        s_labels.append(1)\n",
    "    else: s_labels.append(0)\n",
    "\n",
    "#Now lets train a model on this\n",
    "images = pcna_crops_flat_pad\n",
    "values = s_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, values, test_size=0.4, random_state=42)\n",
    "reg = LinearRegression().fit(X_train,y_train)\n",
    "predictions = np.round(reg.predict(X_test)).astype(int) #rounding the values to 0 or 1\n",
    "correct = np.count_nonzero(y_test == predictions)\n",
    "print('Accuracy:',correct*100/len(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.60836050993181\n"
     ]
    }
   ],
   "source": [
    "#Flat PCNA images to just predict G2/M\n",
    "\n",
    "g2_m_labels = [2 if label==2 else 0 for label in cell_labels]\n",
    "\n",
    "#Now lets train a model on this\n",
    "images = pcna_crops_flat_pad\n",
    "values = g2_m_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, values, test_size=0.4, random_state=42)\n",
    "reg = LinearRegression().fit(X_train,y_train)\n",
    "predictions = np.round(reg.predict(X_test)).astype(int) #rounding the values to 0 or 1\n",
    "correct = np.count_nonzero(y_test == predictions)\n",
    "print('Accuracy:',correct*100/len(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59819 49162 17505\n"
     ]
    }
   ],
   "source": [
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "for label in cell_labels:\n",
    "    if label == 0: count_0+=1\n",
    "    if label == 1: count_1+=1\n",
    "    if label == 2: count_2+=1\n",
    "print(count_0,count_1,count_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6325,)\n",
      "Accuracy: 64.300395256917\n"
     ]
    }
   ],
   "source": [
    "#Lets make an MLP that aggregates these three models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "images = pcna_crops_flat_pad\n",
    "\n",
    "#train1: for G1,S,G2 reg models\n",
    "#test2: split for 3.train mlp with reg model predictions 4.test entire model\n",
    "\n",
    "X_train_1, X_test_2, y_train_1, y_test_2 = train_test_split(images, cell_labels, test_size=0.5, random_state=1)\n",
    "X_train_3, X_test_4, y_train_3, y_test_4 = train_test_split(X_test_2, y_test_2, test_size=0.1, random_state=1)\n",
    "\n",
    "#First we get the data for each model\n",
    "g1_labels = [1 if label==0 else 0 for label in y_train_1]\n",
    "s_labels = [1 if label==1 else 0 for label in y_train_1]\n",
    "g2_m_labels = [1 if label==2 else 0 for label in y_train_1]\n",
    "\n",
    "#Then we train each model on the X_train\n",
    "g1_mod = LinearRegression().fit(X_train_1,g1_labels)\n",
    "s_mod = LinearRegression().fit(X_train_1,s_labels)\n",
    "g2_m_mod = LinearRegression().fit(X_train_1,g2_m_labels)\n",
    "\n",
    "#Then we make it make predictions on the X_test (we want the confidence not binarised, although we could try binarised after)\n",
    "preds_g1 = g1_mod.predict(X_train_3)\n",
    "preds_s = s_mod.predict(X_train_3)\n",
    "preds_g2_m = g2_m_mod.predict(X_train_3)\n",
    "preds = np.dstack((preds_g1,preds_s,preds_g2_m)) #aggregate these predictions into np arrays of 3 values each\n",
    "\n",
    "#Then we train an MLP on this\n",
    "mlp = MLPClassifier(random_state=1, max_iter=300).fit(preds.squeeze(), y_train_3)\n",
    "\n",
    "#Then we predict for entire model\n",
    "#Pipeline starts with predicting from images on the reg models, and using those to make predictions with mlp\n",
    "preds_g1 = g1_mod.predict(X_test_4)\n",
    "preds_s = s_mod.predict(X_test_4)\n",
    "preds_g2_m = g2_m_mod.predict(X_test_4)\n",
    "preds = np.dstack((preds_g1,preds_s,preds_g2_m))\n",
    "whole_predictions = mlp.predict(preds.squeeze())\n",
    "\n",
    "print(whole_predictions.shape)\n",
    "correct = np.count_nonzero(y_test_4 == whole_predictions)\n",
    "print('Accuracy:',correct*100/len(whole_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.1659931831758\n"
     ]
    }
   ],
   "source": [
    "#What if instead of using an MLP classifier we just took the most confident one\n",
    "\n",
    "images = pcna_crops_flat_pad\n",
    "\n",
    "#train1: for G1,S,G2 reg models\n",
    "#test2: split for 3.train mlp with reg model predictions 4.test entire model\n",
    "\n",
    "X_train_1, X_test_2, y_train_1, y_test_2 = train_test_split(images, cell_labels, test_size=0.5, random_state=1)\n",
    "X_train_3, X_test_4, y_train_3, y_test_4 = train_test_split(X_test_2, y_test_2, test_size=0.1, random_state=1)\n",
    "\n",
    "#First we get the data for each model\n",
    "g1_labels = [1 if label==0 else 0 for label in y_train_1]\n",
    "s_labels = [1 if label==1 else 0 for label in y_train_1]\n",
    "g2_m_labels = [1 if label==2 else 0 for label in y_train_1]\n",
    "\n",
    "#Then we train each model on the X_train\n",
    "g1_mod = LinearRegression().fit(X_train_1,g1_labels)\n",
    "s_mod = LinearRegression().fit(X_train_1,s_labels)\n",
    "g2_m_mod = LinearRegression().fit(X_train_1,g2_m_labels)\n",
    "\n",
    "#Then we make it make predictions on the X_test (we want the confidence not binarised, although we could try binarised after)\n",
    "preds_g1 = g1_mod.predict(X_train_3)\n",
    "preds_s = s_mod.predict(X_train_3)\n",
    "preds_g2_m = g2_m_mod.predict(X_train_3)\n",
    "preds = np.dstack((preds_g1,preds_s,preds_g2_m)) #aggregate these predictions into np arrays of 3 values each\n",
    "\n",
    "#For the preds, get the index of the max value and that the prediction\n",
    "preds = preds.squeeze()\n",
    "phase_preds = []\n",
    "for pred in preds:\n",
    "    phase_preds.append(np.int32(np.argmax(pred)))\n",
    "#print(np.array(phase_preds) == y_test_4)\n",
    "correct = np.count_nonzero(y_train_3 == phase_preds)\n",
    "\n",
    "print('Accuracy:',correct*100/len(phase_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.55292024903646\n"
     ]
    }
   ],
   "source": [
    "#Using gradient boosting classifier just for g1\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "g1_labels = [1 if label==0 else 0 for label in cell_labels]\n",
    "\n",
    "#Now lets train a model on this\n",
    "images = pcna_crops_flat_pad\n",
    "values = g1_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, values, test_size=0.4, random_state=42)\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "predictions = np.round(gbc.predict(X_test)).astype(int) #rounding the values to 0 or 1\n",
    "correct = np.count_nonzero(y_test == predictions)\n",
    "print('Accuracy:',correct*100/len(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#What if we made a binary confidence classifier as we did with PCNA but for each channel\n",
    "#So we flatten the crops of each channel and we train a model to predict the phase on each\n",
    "#Then, we aggregate the average confidence for each phase and the most confident is the phase\n",
    "\n",
    "#We should first try making an ensemble learning from the flattened of each phase just with the g1 phase as a binary classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def all_chans_flat_pred_one_phase(crops,labels):\n",
    "    pcna_crops = crops[0]\n",
    "    dapi_crops = crops[1]\n",
    "    edu_crops = crops[2]\n",
    "    cyclina2_crops = crops[3]\n",
    "\n",
    "    #PCNA model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pcna_crops, labels, test_size=0.4, random_state=42)\n",
    "    pcna_mod = LinearRegression().fit(X_train,y_train)\n",
    "    print('PCNA accuracy:',np.count_nonzero(y_test == np.round(pcna_mod.predict(X_test)).astype(int))*100/len(X_test))\n",
    "    conf_pcna = pcna_mod.predict(X_test)\n",
    "\n",
    "    #DAPI model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dapi_crops, labels, test_size=0.4, random_state=42)\n",
    "    dapi_mod = LinearRegression().fit(X_train,y_train)\n",
    "    print('DAPI accuracy:',np.count_nonzero(y_test == np.round(dapi_mod.predict(X_test)).astype(int))*100/len(X_test))\n",
    "    conf_dapi = dapi_mod.predict(X_test)\n",
    "\n",
    "    #EdU model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(edu_crops, labels, test_size=0.4, random_state=42)\n",
    "    edu_mod = LinearRegression().fit(X_train,y_train)\n",
    "    print('EDU accuracy:',np.count_nonzero(y_test == np.round(edu_mod.predict(X_test)).astype(int))*100/len(X_test))\n",
    "    conf_edu = edu_mod.predict(X_test)\n",
    "\n",
    "    #CyclinA2 model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cyclina2_crops, labels, test_size=0.4, random_state=42)\n",
    "    cyclina2_mod = LinearRegression().fit(X_train,y_train)\n",
    "    print('CyclinA2 accuracy:',np.count_nonzero(y_test == np.round(cyclina2_mod.predict(X_test)).astype(int))*100/len(X_test))\n",
    "    conf_cyclina2 = cyclina2_mod.predict(X_test)\n",
    "\n",
    "    conf_tot = []\n",
    "    for i in range(len(conf_pcna)):\n",
    "        conf_tot.append(mean([conf_pcna[i],conf_dapi[i],conf_edu[i],conf_cyclina2[i]]))\n",
    "    conf_tot_round = np.round(conf_tot)\n",
    "    overall_acc = np.count_nonzero(y_test == conf_tot_round.astype(int))*100/len(X_test)\n",
    "\n",
    "    print('Overall acc:',overall_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCNA accuracy: 74.21879632374741\n",
      "DAPI accuracy: 72.91431959679811\n",
      "EDU accuracy: 65.61122640577132\n",
      "CyclinA2 accuracy: 75.30190730309319\n",
      "Overall acc: 78.04526138946537\n"
     ]
    }
   ],
   "source": [
    "g1_labels = [1 if label==0 else 0 for label in cell_labels]\n",
    "crops = [pcna_crops_flat_pad,dapi_crops_flat_pad,edu_crops_flat_pad,cyclina2_crops_flat_pad]\n",
    "all_chans_flat_pred_one_phase(crops,g1_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCNA accuracy: 70.75798003755312\n",
      "DAPI accuracy: 61.74325526237771\n",
      "EDU accuracy: 80.22334222749284\n",
      "CyclinA2 accuracy: 67.58770629508845\n",
      "Overall acc: 77.88714299831999\n"
     ]
    }
   ],
   "source": [
    "s_labels = [1 if label==1 else 0 for label in cell_labels]\n",
    "crops = [pcna_crops_flat_pad,dapi_crops_flat_pad,edu_crops_flat_pad,cyclina2_crops_flat_pad]\n",
    "all_chans_flat_pred_one_phase(crops,s_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCNA accuracy: 85.88793359027572\n",
      "DAPI accuracy: 88.62733471686926\n",
      "EDU accuracy: 85.95315742662318\n",
      "CyclinA2 accuracy: 91.53473663405475\n",
      "Overall acc: 87.6509536515466\n"
     ]
    }
   ],
   "source": [
    "g2_m_labels = [1 if label==2 else 0 for label in cell_labels]\n",
    "crops = [pcna_crops_flat_pad,dapi_crops_flat_pad,edu_crops_flat_pad,cyclina2_crops_flat_pad]\n",
    "all_chans_flat_pred_one_phase(crops,g2_m_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.04526138946537\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(y_test == conf_tot_round.astype(int))*100/len(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "#I'd like to plot the values of each cells and color them with their labels\n",
    "\n",
    "#First lets get each DAPI, EdU etc. values\n",
    "def get_crops_flat_pad(df):\n",
    "    pcna_val = []\n",
    "    dapi_val = []\n",
    "    edu_val = []\n",
    "    cyclina2_val = []\n",
    "    dna_val = []\n",
    "    for i in range(len(df)):\n",
    "        pcna_val.append(df['pcna_values'][i])\n",
    "        dapi_val.append(df['dapi_values'][i])\n",
    "        edu_val.append(df['edu_values'][i])\n",
    "        cyclina2_val.append(df['cyclina2_values'][i])\n",
    "        dna_val.append(df['DNA_content'][i])\n",
    "\n",
    "    return pcna_val,dapi_val,edu_val,cyclina2_val,dna_val\n",
    "\n",
    "pcna_val,dapi_val,edu_val,cyclina2_val,dna_val = get_crops_flat_pad(df)\n",
    "\n",
    "#Second lets get the corresponding label values\n",
    "cell_labels = get_cell_labels(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmbUlEQVR4nO3df5xcdX3v8dfZSbJsNpuElIiaXS7XKp96UWkTq7EXNbW2NFRq6t5HHhAVf4AiXAFNe1FYMDeSldZKlItCVUSlNPAI3QeWRxWKV8QSCg8u5Hovcu2HoqXZTZWHEBKWZNkks3P/mDPJZHbO/NidM3PO2ffz8ciD3e85M/OZ2eFzzvl8v+f7DQqFAiIiki1dnQ5ARERaT8ldRCSDlNxFRDJIyV1EJIOU3EVEMkjJXUQkg+bV28HM5gM3AycD3cAWd7+rbPsngfOBX4VNFwC7gFuBlwHjwAfc/VfUMDU1Vcjno4dl5nIBtbYnTdrihfTFrHjjpXjj1ap458/PPQssr2yvm9yB9wHPufv7zWwZ8GPgrrLtq4Bz3f2xUoOZbQQed/f/bmZnA1cCl9Z6kXy+wN69ByK3L126sOb2pElbvJC+mBVvvBRvvFoV7/Llff9Wrb2RsswdwFXhzwFwuGL7KuByM9thZpeHbacD94Q/3w28s7lwRURkNuqeubv7iwBm1gf8LcWz8HK3A18BXgDuNLN3AYuBfeH2cWBJvdfJ5QKWLl1YY3tXze1Jk7Z4IX0xK954Kd54xR1vI2UZzGwAuBO4wd23lbUHwJfcfV/4+3eB36KY6PvC3fqAvfVeQ2WZzktbzIo3Xoo3Xi0sy1Rtb6RD9UTgXuDj7v6Dis2LgZ+Y2WuB/cA7KHa+HgDOBB4B1gIPzDhyERFpWiNn7lcAxwNXmVmp9v51oNfdv2ZmVwA/BCaBH7j798zsfuDbZrYDOAhsaH3oIiISJUjKrJCHDuULKst0VtpiVrzxUrzxunvsOwzdN8TuF8dYsaifodWbGDxlfdPPs3x532PAGyvbG6q5i4hI64w8uZ0/vf8SDhwuHozGXhxl4/0XA8wowVejO1RFRNps+OHNRxJ7ycThCYYf3tyy11ByFxFps90vjjXVPhNK7iIibbZiUX9T7TOh5C4i0mZDqzexcN6xNzD1zOthaPWmlr2GkruISJsNnrKeG8/8K/oXDRAQ0L9ogK1rrm9ZZypotIyISEec87oNrO1fF9vz68xdRCSDlNxFRDJIyV1EJIOU3EVEMkjJXUQkg5TcRUQySMldRCSDlNxFRDJIyV1EJIOU3EVEMqjm9ANmNp/imqgnA93AFne/q2z7OcAngMPA48BF7j5lZjspLpIN8K/u/qHWhy4iIlHqzS3zPuA5d3+/mS0DfgzcBWBmPcAW4PXufsDMbgPeZWb3AoG7r4kvbBERqaVecr8D+Nvw54DiGXrJJPA77l5aTmQe8BJwGrAwTPLzgCvc/eHWhSwiIvU0tEC2mfVRPGP/urtvq7L9YuDM8N/rgNXATcBrgLsBc/fDlY8rNzU1Vcjno2PJ5brI56fqxpoUaYsX0hez4o2X4o1Xq+KdPz83swWyzWwAuBO4oTKxm1kX8HngFGDQ3Qtm9iTwlLsXgCfN7DngFcBordfJ5ws1Vy5P28rmaYsX0hez4o2X4o1Xq+Jdvryvanu9DtUTgXuBj7v7D6rs8lWK5Zl17l46BH0YeD1wkZm9ElgM/GKGcYuIyAzUO3O/AjgeuMrMrgrbvg70Ao8C5wEPAPeZGcB1wDeAb5nZDqAAfLheSUZERFqrZnJ390uBS2vsEjVOfsOMIxIRkVnTTUwiIhmk5C4ikkFK7iIiGaTkLiKSQUruIiIZpOQuIpJBSu4iIhmk5C4ikkFK7iIiGaTkLiKSQUruIiIZpOQuIpJBSu4iIhmk5C4ikkFK7iIiGaTkLiKSQUruIiIZ1MgC2fOBm4GTgW5gi7vfVbb9LOAzwGHgZnf/upn1ALcCLwPGgQ+4+69aH76IiFTTyJn7+4Dn3P2twB8CXy5tCBP/F4E/AN4OfDRcVPtC4PHwMbcAV7Y6cBERidZIcr8DKC2OHVA8Qy95LfCUuz/v7geBHcDbgNOBe8J97gbe2ZpwRUSkEXXLMu7+IoCZ9QF/y7Fn4YuBfWW/jwNLKtpLbTXlcgFLly6ssb2r5vakSVu8kL6YFW+8FG+84o63bnIHMLMB4E7gBnffVrbpBaCv7Pc+YG9Fe6mtpny+wN69ByK3L126sOb2pElbvJC+mBVvvBRvvFoV7/LlfVXb65Zlwhr6vcCn3P3mis0/BV5jZsvMbAHFksxDwIPAmeE+a4EHZhh3w0ZG5rFyZS8nnriIlSt7GRlp6LglIpJJjWTAK4DjgavMrFR7/zrQ6+5fM7ONwD9QPFDc7O67zexG4NtmtgM4CGyIIfYjRkbmsXHjcUxMBACMjQVs3Hgc8BKDg4drP1hEJIOCQqHQ6RgAOHQoX5hpWWblyl7GxqZfhPT3T7Fz5/6WxdiMtF0iQvpiVrzxUrzxamFZ5jHgjZXtmbiJaffuoKl2EZGsy0RyX7Gi+tVHVLuISNZlIrkPDU3S03NsIu/pKTA0NNmhiEREOisTyX1w8DBbt75Ef/8UUCCXKzAxAcPD3Ro1IyJzUmYyX2lUjEbNiIhk5My9ZHi4+0hiL5mYCBge7u5QRK2hMfwi0qxMZYksjprRGH4RmYlMnblncdRMVq9GRCRemUruWRw1k8WrERGJX6aSe/momSAo0N8/xdat6S5fZPFqRETil6maOxQTfJqTeaWhocljau6Q/qsREYlfps7csyiLVyMiEr/MnblnUdauRkQkfjpzFxHJICV3EZEMUnIXEckgJXcRkQxqdIHsNwN/4e5rytpeDtxetttvAp8GvgqMAf8Stj/k7pe3IlgREWlM3eRuZpcB7weOWa/O3X8JrAn3eQswTHFt1V8Hdrr7Wa0OVkREGtNIWeZnwHuiNppZAFwPXOjueWAVsMLMfmhm3zMza02oIiLSqLpn7u4+YmYn19jlLOAJd/fw918A17j7HWZ2OnAr8Nv1XieXC1i6dGGN7V01tydN2uKF9MWseOOleOMVd7ytuInpfcB1Zb8/ChwGcPcdZvZKMwvcveZkKPl8oeZK4HN1ZfN2SlvMijdeijderYp3+fK+qu2tGC3zRuCfyn7fBHwCwMxOA0brJXYREWmtps/czWwDsMjdv2Zmy4EXKpL3nwO3mtkfUTyD/2BLIhURkYY1lNzd/WlgdfjztrL2X1EcAlm+7/PAH7UsQhERaZpuYhIRySAldxGRDFJyFxHJICV3EZEMUnIXEckgJXcRkQxSchcRySAldxGRDFJyFxHJICV3EZEMykRyHxmZx8qVvZx44iJWruxlZKQVk12KiKRX6rPgyMg8Nm48jomJAICxsYCNG48DXmJw8HBngxMR6ZDUn7kPD3cfSewlExMBw8PdHYpIRKTzUp/cd+8OmmoXEZkLUp/cV6yovg5IVLuIyFyQ+uQ+NDRJT8+xibynp8DQ0GSHIhIR6bzUJ/fBwcNs3foS/f1TBEGB/v4ptm5VZ6qIzG0NjZYxszcDf+HuayraPwmcD/wqbLoA2AXcCrwMGAc+EK7YFJvBwcNK5iIiZeqeuZvZZcBNwHFVNq8CznX3NeE/By4EHnf3twK3AFe2MmCRWkae3M7KW07lxBuWsPKWUxl5cnunQxLpiEbKMj8D3hOxbRVwuZntMLPLw7bTgXvCn+8G3jm7EEUaM/LkdjbefzFjL45SoMDYi6NsvP9iJXiZk+qWZdx9xMxOjth8O/AV4AXgTjN7F7AY2BduHweWNBJILhewdOnCGtu7am5PmrTFC+mLuTLeax75LBOHJ47ZZ+LwBNc88lnOe9MH2xzddGn/fJNO8R5rxneomlkAfMnd94W/fxf4LYqJvi/crQ/Y28jz5fMF9u49ELl96dKFNbcnTdrihfTFXBnv6AujVfcbfWE0Ee8r7Z9v0s3VeJcv76vaPpvRMouBn5jZojDRvwN4DHgQODPcZy3wwCxeQ6RhKxb1N9UukmVNJ3cz22BmHw3P2K8AfkgxgT/h7t8DbgRONbMdwEeBza0MWCTK0OpN9MzrOaatZ14PQ6s3dSgikc4JCoVk3Ml56FC+oLJMZ6Ut5mrxjjy5neGHN7P7xTFWLOpnaPUmBk9ZH9ne6XiTTPHGq4VlmceAN1a2p35WSJFyg6esn5a0S6NoSp2tpVE0pf2TKAkHI0m31N+hKlLP8MObq46iGX44mRVDDemUVlByl8zb/eJYU+2dlraDkSSTkrtkXtpG0aTtYCTJlJnk3j2ynWUrT+WEE5ewbOWpdI/oElaK0jCKpnzahK6g+v+WST0YSTJlokO1e2Q7fRsvJpgoXsrmxkbp21jsMJscVCfUXFfqiExqB2Vlh2++kJ+2T9IORpJ8mUjuvcObjyT2kmBigt7hzUruAlQfRZMU1WrsALkgx1RhKnEHI0mHTCT3rt3Va5FR7dJa3SPb6R3eTNfuMaZW9LN/aJMOqk2IqqVPFaZ45qJ9VbeJ1JOJmvvUiuq1yKh2aZ1SSSw3NkpQKBwpianPo3Fp6/CVdMhEct8/tIlCz7EdZoWeHvYPqUYZt1olMWlMGjp8JX0ykdwnB9czvvV68v0DFIKAfP8A41uvV2mgDVQSm73BU9azdc319C8aICCgf9EAW9dcrxq7zEomau5QTPBK5u03taKf3Nj0qXZVEmtOkjt8JZ0yceYunaOSmEgyZSq560am9lNJTCSZMpPcNWqjcyYH17Nn5xM8+8w+9ux8omZib+cBWItly1yWmeSuURvJ184DsGZWlLkuM8ldozY6o5kz8XYegDWzosx1DY2WMbM3A3/h7msq2s8BPgEcBh4HLnL3KTPbSXGhbIB/dfcPtSziCBq10Zxqd5Vy3gebfo5m5vRp5wFYMyvKXFf3zN3MLgNuAo6raO8BtgC/6+7/GVgCvMvMjgMCd18T/os9sYNGbTQjqjwS3Latqedp9ky8nXcS665PmesaKcv8DHhPlfZJ4HfcvbQI4DzgJeA0YKGZ3Wtm95nZ6taEWptGbTQuKinnrrqyqedp9ky8nQdg3fUpc13dsoy7j5jZyVXap4BnAMzsYmAR8H3gdcAXKJ7tvwa428zM3Q/Xep1cLmDp0oU1tnfV3A4QLFxAV1cAQFdXwMKFC+ip85i4NBJvp0SWQUZHm4t5YAB27araXvV5zvsg+YULigeR0VEYGCB/9RZ6ztlAz/S966r1GZ/3pg+ycOECrrr/SkZfGGVg8QBXr9nCOa/b0NBz3/aTbTN+7EziTSLFG6+44w0KhULdncLkfru7r65o7wI+D5wCnO3uB8ysG+hy94lwn0eAQXefXhAvc+hQvlBrJfB6K4VX1n+heFbYqbP3JK/EvmzlqVX7JwonncSzj/6k4efp9Gce12dcOb86FM/6ZzslQJK/E9Uo3ni1Kt7ly/seA95Y2T7b0TJfpViLX1dWnvkwcC2Amb0SWAz8YpavU1X5SI2+j1+goZANiiqP5K/e0tTztKsUFufY+Gpj4TXSRrKg6bllzGwDxRLMo8B5wAPAfWYGcB3wDeBbZrYDKAAfrleSmYlpZ4356avXwNwcCllvfvXSz5X79JyzAZo8k4h7Tp9aI3KaHd1TqfIMvTQWvtrCGaCRNpIuDZVl2qHZskxUaaFSvn+APTufaEmMzejUJeJsSiVJvKyN+jvn+weY+vm/zirelbecytiL0587F+SqLnXXv2iAnefO/LuUxM+3FsUbr6SXZTqmkTPyuTgUMmt36sY5Nj7qTDxfyGukjaReapN71NjoQi43p4dCZu1O3TjHxkeNeS/Np6751SXNUpvcozoFx7/81YYmsGqFJM5COZNkWHof87rnJ+Z9lMQ5Nr7WWPjBU9az89wneOaifew89wkldkmd1Cb3Tt+0lNRZKJtNhkl9HyVx/p2rrYB0tr2X4Yc3ayZJSb3Udqh2Wq2Ovj07n+hovPVGy5Sr9z6SrNWfcVzj20uS9h2uR/HGSx2qDege2c6v/cbJnPCyxZzwssX8mp0c+5lnkmvbzcyvnuT3UU15KWzeq1/V0r+zxrdLlqQ+uXePbKfv0ovo2rOHAAiAruf30HfJhbEm+HZOgtVKlf0EheOPr7pfEt9HZQkp2LWL3CXns/nSk2uWTxpdtEMzSUqWpD659w5vJjh4cFp7cOhQrMP/0jgLZbX6ejA+TmHBgmP2a8f7mElndLVhnr2H4M++tydyIY5mFu3QTJKSJalP7l01bmSKs7TQ6Q7dmag6Bv7QIQq9i9r6PmbaiRv19zxpX3T5pJlSi2aSlCxJfXKnK/otxF1aaKa2PVutGHYZlRyDvc+zZ+cTHJ481JYhpDO90Srq77lrSfG/1conzZRaqo2e0fh2Saum55ZJnKmpqs0FSHSJpBnNrngUJSmrVc20E3f/0KZpUyvsnw9X/F7x52rlkxWL+qtOMRBVahk8Zb2SuWRC+s/ca0hyiaQZrZpSoN39BFFXG410Rld7bKkUNn7iMqaAp5fAR86C294QXT5RqUXmqtQn98KyZU21NyMpd6C2arhiO/sJatXV6x1kaj12cnA9Lz3+NF/bcRNv33QSt7+hdvlEpRaZq1J/E1NpKGT5iJnCggWMX3fDrJLWbBeiaOUNFe260aidMde60arR9ztXb1ppF8UbL93EVMfk4HrGr7vh2LPRWSZ2SNbsimkcdlnvaqNWZ3TUCKik3lglkkSpT+4Qz6iVJN25mcRhl/VKVjO9yat7ZDsEwYweKyJHZSK5xyFpd6C2c9hlPY2MU5/p1Ubv8GaCKqXCAhAc2N/x/g+RtGgouZvZm83s/irtZ5nZ/zKzh8zsI2Fbj5mNmNkDZvY9M1ve4pir6v3URk54xfHF+WVecTy9n9o4q+dLYymkXRopWTVztVF+FVDzprQ9e445mAS3bWvdmxLJmLrJ3cwuA26iuBB2eft84IvAHwBvBz5qZicCFwKPu/tbgVuAK1sddKXeT22k55s3EeTzxfll8nl6vnnTrBJ8Ekshceoe2c68V7+qoTPjRktWjVxtTJsvJuI1K9uDiQlyV8X+1RJJrUbO3H8GvKdK+2uBp9z9eXc/COwA3gacDtwT7nM38M5WBFpLzy3fnP4/f9g+G0kqhcTpyMigXbsamg6glSWralcBlSLHc43WX0NXZK6qe4equ4+Y2clVNi0G9pX9Pg4sqWgvtdWVywUsXbqwxvauqtuD27ZBfvpixgDk8zWfM05R8bZLcNu24pnt6CgMDJC/eguFczZU3XfeNZ+tWmbpu+az9Jz3wWn7F4aHKVz4MYIDR4dxFRYupDA83PR7jroKKECxY3VgAPbvh+eem77TwEBHP+Nmdfo70SzFG6+4453N9AMvAH1lv/cBeyvaS2115fOFmmM+q40JPXLGGfWgXK5j4147vVjHMWP0d+0i97ELGD9wsOrVxwlRZ8Cjo9Xfw9p1dF97cPo49bXroMn3vCxqSoSyMe1R9xzkr94yJ8c1t4vijVcLx7lXbZ/NaJmfAq8xs2VmtoBiSeYh4EHgzHCftcADs3iNmmpd0heAiXM/FNdLJ1qzY/RnUmZptmQVNXSykY7rqP6PqCsREZnBmbuZbQAWufvXzGwj8A8UDxI3u/tuM7sR+LaZ7QAOArH9H1hrZAVdXRx+0+q4XjrRmh2jX21CrlaODKq8izg3NkrfpRcBR+f/qbcs4OTg+mltxx4SRKRcqqcfOOHlSwkiZoWEzq4D2slLxJlMV9A9sp2+az4Lo6N1111t1JEpBsZGq5bOppYt47l/fnrGzz9XL8PbRfHGS9MP1FIjscPcvV19/9Cm6asrLVhQ80x8cnA9h5/6ectGBh0zxDFin2DPnhk//8iT23n1l19Vd+k8kbkq3cm9jjl9u3rlFVmbr9AaGeJYqdFZOEtL5+16YVfdpfNE5qpUJ/fC8dHT+haAyd8/o33BJEjv8GaCQ4eOaYt7TdlKjVw1FXp7j/zczNJ7zSydJzJXpTq5v/i5z1OYP7/qtgDo/v4/tDeghEjCpGcNXTUt6D7yYzMjfJpZOk9krkp1cp8cXM/4/7gx8g7GuVpzT8KkZ9WGOFYK9j5/5OdmDkhRS+RFtYvMRalO7lBM8FP9A1W3pb3mPtOVoJIw6dkxY9OjdioUjryvyL9V2T4lWjpPpL7UJ3dIRjJrtWZq0JWSMulZ6Uan8RtvqnoWH3B0se/J3z+j7j6l915aOu+kxSdp6TyRCKke517SPbKdRVdcRvB8cWhdYdkyXhz+fEcn+prtGNZ2La1XLs5xwvXGvOf7B9g/tKnuPlpmr30Ub7w0zr2O0hlu1/N7itP9AsH4OIuGLkv1wg5J6BRtpdJZfNQqS127xxraR0Qak/rkXnWUxaFD0xZ2SFuCT0KnaBwaeV9Zfe8i7ZT65N7I2VynFraejaz2IwQH9k/rYK18X1l87yLtlvrk3ujZXNou6ZPSKdqoeiN7jpTP9uw5Uk8vAFPHL5v2vtL23kWSaDbzuSfC/qFN9F14fvSc7qE0XtJXmwkxiSrnWy+VwuDYWR+nlc+Aqd7equ8xLe9dJKlSf+Y+ObiewrLoaQhAl/Rxq3V3aemMPmp65q6x0dT1h4ikQeqTO8CLw5+fNgsihJf9y6Zf9ktrRY7sCc/ga84MCfRd9JFZLWYuItNlIrlPDq6n0LtoWnsAFBZWv+xPk5neqdoukSWvXK6hmSGDQoGeb30jce9LJM0ykdzh2HlKyqWtI7XSbO5UbUdspZJLoWJseqGnJ3rh8iqCQuHIiKakH8xE0qBuh6qZdQE3AKcBk8D57v5UuO03gS+V7b4aWAc8AjwJ/CRsv9Pdr2tV0NVMRS20nMKO1HK16tlxXZEcuZu0xrJ30xatLhSKCb5QYKrsbtNqf5MoXbvHGuqcFZH6GjlzXwcc5+5vAT4NXFva4O4/dvc17r4G+Aow4u73ACuB20rb4k7sEDE2OghSP6d7o3eqtupst9ErhaoHnTCxl1ZyamRmyHJTK/qbXtxbRKprJLmfDtwD4O4PU2UOAzPrBTYDl4ZNq4BVZvYjM7vDzF7RongjTQ6uZ+Ls9x5THggKBXpu/5tUX9Y3crdmK0s3jSbXWp2oJZXj1aeWLWPq+GUUoOqNTJO/f0b0qJqUl9dE2q3uxGFmdhPFM/K7w993Aa9y98Nl+1wC/Jq7bwp/fzew393/p5m9F/gTd/8vtV5namqqkM9Hx5LLdZHP114zdd6rX0Wwa9e09sJJJ3H4qZ/XfGyrNRJvI4LbtpG78GMEB45OMFRYuJD8jX9F4ZwNQOvedy7XRTAvR1DlO1EIAg5PHl3dKfI1g4D8t759JLaa7+uqK2F0FAYGmFp7Jl1/fcsx77Pee2nVZ9wuijdeczXe+fNzVScOa+QmpheAvrLfu8oTe+i9QHnyvg8o/V96J/DZei+SzxdqzpDWyAxqJ4xG1HdHR1sy+1ojteiSZmd8i3zutevovvbg9G1r10H4/K1630uXLqSrRt9F+XN1X/4Z+i76yLQDQVAoEAwN8fzadbVfbO264r/QspWnRif2nh7GL/8MkxXvZa7OAtguijdeLZwVsmp7I2WZB4EzAcxsNfB4+UYzWwJ0u3t5RrgJGAx//j3gsSbjnZE4J5yKc9RKvecuzZb47DP7jtSzy7XyfUf1XXSNjR5Ty58cXB+56Hblvo2IKrsUQPcpiMxAI8n9TuAlM/sn4IvAJ81so5n9cbj9FODpisd8GrjQzO4HPsbRWnws6g3Ja8XdqXF29M32uVs50VblCkqFICiejTN90YyoFbCq7VtP5AGqf0CJXWQGUr9Yx7QheTBtSF4rksMJJy6JrEU/+8y+huMtj7tUaiFMno0+d73nq1cyilIZc70FQ6p99lH7NhL/tL9jT0/Ns/a5ehneLoo3Xlqso45GhuS1QitLH5VlmKhb85t57srSDXDM0MjeT21seqhkvZErjayT2ugoF80EKdJaqU/u7VqxqJWlj2oHpEqzKSdVq+H3fPOmpvoLuke2R66IVH7QKR1UWrFIeb2+BRFpXOqTe7tW7al2Zjlx9nvpHd7c9I1DtToPZ/vcED297jG/T0zQ9/ELIp+3d3hzZBmq2kFHC2yIJEvqk3s7k0r5meX+oU303P43Mxo9U6vzcLbPDY1ftQT5fNXn7R7ZHlmSoVCInH9dZRWR5Eh9cm9nUim/xb/v4xfMeIRLvQPSbEfPNHPVUvm8Rzo2o547ovwCKquIJEnqV2KC9qzaM200R8SMh42cNZevTlRtdMts+hHK1yktT9CVv0c9b63+AJVZRNIjE8m9HRrpBIXGz5prHZBmOsNl1eGEQOH4ZRx6/RtY8I/3V03w5c+rm4lEsiH1ZRloz/zfjZw1t+rMdqb9CFEdqYXeXub9/GeRY+nLn1c3E4lkQ+qTe7sWs4hKeoVcruW1/pn2I9Qq50QenCo6SDXqRSQbUp/c2zX/d1TSG//yV2PpQJxJ52StYaG1zsgrX1ejXkTSL/U193bdxFSvEzQJ9g9tqnoLf+msu9a2cu3ooBaReKU+ubdzeb2kJ71GDkBJPjiJSOukPrnvH9pE3yUXEhw6upBEYf78OVsjrnUASvrBSURaJ/U1d2D6HCgRc6KkSTtGAIlIdqU+ufcObyY4ePCYtuDgwVQvqNzMCCAdBESkmtQn93Z1qLZToyOA2jUMVETSJ/XJvV2zQrZTowesdg0DFZH0SX1yz+JNN40esLJ41SIirVF3tIyZdQE3AKcBk8D57v5U2fbrgNOB8bDp3cB8YBvQA/w78CF3j2X9qzSMP29WvfHqJe0cBioi6dLImfs64Dh3fwvFha+vrdi+CjjD3deE//YBnwG2uftbgf8NXNDCmKfJ2lSzjd4lmsWrFhFpjboLZJvZVuARd789/H23u68If+4CfgE8CJwIfMPdbzazncCZ7v5LMzsN+Jy7/1Gt15mamirk89Gx5HJd5PNTTby1zmpXvMFt28hddSWMjsLAAPmrt1A4Z8OMnkufcbwUb7zmarzz5+eqLpDdyE1Mi4F9Zb/nzWyeux8GeoHrga1ADvihmT1a8ZhxYEm9F8nnCzVXAp+rK5vXtXZd8V+5Gb6uPuN4Kd54zdV4ly/vq9reSFnmBaD80V1hYgc4AFzn7gfcfRy4j2JtvvwxfcDeGcQsNWh8u4jU0khyfxA4E8DMVgOPl207BXjQzHJmNp9ix+rO8scAa4EHWhaxaHy7iNTVSHK/E3jJzP4J+CLwSTPbaGZ/7O4/Bf4aeBj4EXCLuz8BbAHONrMHgbcAX44n/LlJ49tFpJ66NXd3nwI+VtH8z2Xb/xL4y4rHPAP8YSsClOk0vl1E6kn9TUxzURbvyhWR1lJyTyGNbxeRepTcU0hL4YlIPalfrGOu0sIbIlKLztxFRDJIyV1EJIOU3EVEMkjJXUQkg5TcRUQyqO6Uv230K+DfOh2EiEjK/AdgeWVjkpK7iIi0iMoyIiIZpOQuIpJBSu4iIhmk5C4ikkFK7iIiGdSxicPMrAu4geKaq5PA+e7+VNn2jwAXAIeBLe7+92Z2ArAN6AH+HfiQux+otm9C4j0JuJni5xwAH3V3N7NPAudTHP4JcIG7e0JiXgY8Cfwk3O1Od78uwZ/xl4DfDHd5ObDX3Veb2XUUl30cD7e9293LF3qPPd5wn+UUl518g7u/ZGY9wK3Ay8LYPuDuvzKzs4DPhO/tZnf/eitjnUW8S8J4FwMLgI3u/pCZ/QnwBWA0fOgmd/9RAuINgDHgX8JdHnL3yxP8+X6aowsbLQVe7u4vb0WO6OSskOuA49z9LeHarNcC7wYws5cDlwBvBI4DdpjZ9yn+cba5+7fCD+UCM7ut2r7uPpmAeK8Gvuzu3zGzM4BrgPcAq4Bz3f2xFsfYiphXAre5+8WlJ4naNwmfsbt/Itw+H9gBfCR8rlXAGe7+bItjbCjeMKYzgD+neNApuRB43N3/u5mdDVxpZn9GcQnL3wb2U1yX+K5wRbNOx7sR+IG7f8nMDLiN4ndkFXCZu4+0OMbZxvvrwE53P6tsv/kk9PN19z8P2zCzvwcuCzfNOkd0sixzOnAPgLs/TPF/2pI3AQ+6+2R4tvUU8IbyxwB3A++ssW8S4v1T4LvhPvOAl8KfVwGXm9kOM7s8hlhnE/MqYJWZ/cjM7jCzV9TYNwnxllwM3Ovuj4dnUK8BvmZmD5rZh2OItV68AFMUv6N7qj2Go9/h1wJPufvz7n6Q4kHqbQmJ94vAV8OfK7/DHzazB8zsWjOL40RxJvGuAlaY2Q/N7HvhASnJny8AZvYe4Hl3v7fsfcwqR3QyuS8Gyi+T82VfkMpt48CSivZqbeXtHY/X3Z9190PhF+wLQGkF69sprkv7DuB0M3tXDPHOKGaK6+N+xt3fDnwHuL7GvkmIFzNbQLFc84VwWy/FuN9H8ZL3IjOL42BUK17c/fvu/lyNxyTpO1w1Xnff6+4T4ZXTrUAp0Xyf4gH1bcAipq+z3JF4gV8A17j77wKf42hJKZGfb5nLOZofoAU5opPJ/QWgr+z3Lnc/HLGtD9hb0V6trby91WYSL2b2uxST5PvDensAfClM/Acpntn/VgzxzjTm+4Afhm13hrEl+jOmeDb0j2U19QPAde5+wN3HKb6n09ocbyOPSdJ3OJKZvR74AXBFWV39Znf/ubsXgL8jnu/wTOJ9NIwHd98BvJJiMk/y5/ufKPYVPRX+3pIc0cnk/iBwJkBYn3q8bNsjwFvN7LiwQ+e1FDv4jjwGWAs8UGPfjscbJvbrgD9090fDfReH2xaFf8R3AHHV3mfyGd8EDIb7/F4YW2I/43DbOymWOEpOoVhXzYX11tOBnW2Ot+5jOPod/inwGjNbFl6FvA14qPXhNh9vmHjuADa4+91hWwD8XzMrrche+p50PF5gE/CJ8DGnUezw/X8k9PMNVX5/W5IjOja3TFnP8hsojiT5EMUP5il3v8uKIyM+SvEA9Dl3HzGzE4FvUzw6PkvxC7e/2r4Jiff/AN3AL8OncXe/wMzeT7FzcJJiZ1UsK1vPMOb/SHGET0Cx8+l8d/9FUj/j8HHfBYbc/cdlz/XfgPXAIeAWd/+rdsdbtt/TwG+EoyMWUvwOvwI4SPE7/Es7Opqji+JZ8VcSEu/fUbzqeTrcvM/d321mfwBsASYoJs9L3P1QAuI9nmIpZhHFkTH/1d3/Oamfb/j7V4Dvu/t3yvaZdY7QxGEiIhmkm5hERDJIyV1EJIOU3EVEMkjJXUQkg5TcRUQySMldRCSDlNxFRDJIyV1EJIP+P1+BZhJjOHSYAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Scatter plot each with a color\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = {0:'red', 1:'green', 2:'blue'}\n",
    "#cell_labels_arr = list(cell_labels)\n",
    "for i in range(len(pcna_val[:100])):\n",
    "    plt.scatter(pcna_val[i],dna_val[i],c=colors[cell_labels[i]])\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(pcna_val[:100])):\n",
    "    plt.scatter(dapi_val[i],dna_val[i],c=colors[cell_labels[i]])\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(pcna_val[:100])):\n",
    "    plt.scatter(edu_val[i],dna_val[i],c=colors[cell_labels[i]])\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(pcna_val[:100])):\n",
    "    plt.scatter(cyclina2_val[i],dna_val[i],c=colors[cell_labels[i]])\n",
    "plt.show()\n",
    "#plt.scatter(pcna_val,dna_val,c=colors[cell_labels_arr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "array([20.9811195 , 21.2705751 , 22.07082885, 23.0711454 , 22.94344395,\n       22.0410321 , 23.08817175, 22.54331835, 23.30100495, 23.3648544 ,\n       22.68378765, 23.7947793 , 24.05869155, 23.5095822 , 21.9814386 ,\n       23.9948421 , 23.91822225, 23.9607894 , 23.2031028 , 22.31771475,\n       22.56460065, 22.9051353 , 22.05379995, 22.5731151 , 22.7050725 ,\n       23.3563425 , 22.93067355, 22.36453785, 22.0367736 , 22.37730825,\n       23.5819461 , 24.48861645, 24.28429515, 24.70996155, 25.8337236 ,\n       24.86745975, 24.6461121 , 25.1696832 , 23.50106775, 22.8668241 ,\n       22.98175515, 21.8835339 , 23.61174285, 21.5515137 , 23.415936  ,\n       21.6707007 , 24.64185615, 23.18181795, 21.6025953 , 24.04592115,\n       22.6412205 , 24.13105545, 24.62908575, 24.2332161 , 28.1067834 ,\n       32.5294626 , 31.92927165, 34.25341305, 31.9846092 , 29.7583725 ,\n       25.69751025, 24.8334045 , 23.65856595, 25.4761626 , 22.0027209 ,\n       22.8923649 , 23.5095822 , 23.13499485, 23.34782805, 22.67527575,\n       21.81117   , 23.6415396 , 25.4548803 , 24.05869155, 26.00398965,\n       29.19223425, 39.31885035, 43.1753913 , 40.3872723 , 40.246803  ,\n       38.65480995, 38.4590031 , 34.8067809 , 27.81733035, 25.41231315,\n       22.49649525, 24.1821345 , 25.076037  , 22.7859483 , 23.76498255,\n       23.3989071 , 23.96504535, 22.5943974 , 22.94770245, 23.3435721 ,\n       25.3101525 , 30.2393739 , 38.4334623 , 45.3803457 , 45.1292013 ,\n       45.8570937 , 44.00118585, 40.1148456 , 39.42526695, 39.10175865,\n       33.48295605, 27.6428058 , 23.93099265, 22.9264176 , 23.1051981 ,\n       22.96898475, 21.7090119 , 22.31771475, 21.5983368 , 22.71784035,\n       24.5396955 , 24.9653619 , 30.92469915, 40.0339698 , 45.37608975,\n       48.63669825, 44.93339445, 46.172085  , 47.19794235, 46.21465215,\n       42.5496621 , 38.86764315, 38.47602945, 32.2825767 , 25.9997337 ,\n       24.186393  , 23.88416955, 23.7564681 , 22.9519584 , 23.87991105,\n       22.7859483 , 23.2031028 , 25.41231315, 28.0471899 , 38.9910861 ,\n       46.4572821 , 46.26573375, 45.51655905, 43.39673895, 43.89051075,\n       53.80429365, 50.95658115, 45.878376  , 40.57456725, 39.87221565,\n       34.9855614 , 25.4421099 , 23.3095194 , 23.75221215, 23.16904755,\n       23.57343165, 23.90545185, 23.4585006 , 23.2584378 , 26.672286  ,\n       32.7933774 , 44.3034093 , 50.02437255, 47.17666005, 46.2955305 ,\n       44.3757732 , 45.34203705, 50.381931  , 48.6792654 , 44.03523855,\n       40.723551  , 40.1786976 , 35.00258775, 27.1618044 , 23.97781575,\n       22.83702735, 24.27578325, 22.820001  , 25.5612969 , 25.9997337 ,\n       25.1654247 , 28.9027812 , 36.23276385, 42.8816823 , 46.14654675,\n       46.15505865, 49.33479135, 50.8927317 , 50.07970755, 50.67138405,\n       44.4821898 , 41.7196116 , 41.25137805, 40.42983945, 34.9600206 ,\n       25.69751025, 25.54427055, 24.4119966 , 24.68867925, 22.81574505,\n       24.5396955 , 23.83308795, 27.6428058 , 35.9773635 , 42.69438735,\n       44.05652085, 46.2103962 , 48.2025174 , 50.02011405, 51.9100746 ,\n       52.8720825 , 52.1271663 , 46.12100595, 40.77463005, 40.246803  ,\n       39.5572218 , 34.79826645, 28.0471899 , 23.56066125, 24.067206  ,\n       23.9693013 , 23.9693013 , 24.9313092 , 26.76167625, 30.54159735,\n       39.97863225, 48.3898098 , 47.8577268 , 48.1642062 , 44.03523855,\n       46.21465215, 49.33479135, 51.2034696 , 54.6258297 , 54.7960983 ,\n       44.82697785, 40.0552521 , 38.4334623 , 34.7940105 , 26.54884305,\n       24.6248298 , 23.296749  , 22.60291185, 24.85468935, 25.2250182 ,\n       27.55767405, 35.08346355, 49.1645253 , 52.2080421 , 52.5145215 ,\n       51.86751   , 46.3891767 , 48.6750069 , 46.6956561 , 47.90029395,\n       52.88910885, 53.7532146 , 49.0197975 , 42.30703215, 41.68555635,\n       35.0877195 , 26.1870261 , 25.7485893 , 23.07540135, 23.5691757 ,\n       25.65919905, 27.3490968 , 29.222031  , 36.77761725, 46.7424792 ,\n       50.03288445, 47.3256438 , 49.45397835, 45.50804715, 46.0826973 ,\n       47.80664775, 47.47462755, 50.48409165, 48.41109465, 46.13377635,\n       43.45633245, 44.7971811 , 38.7399417 , 27.00005025, 25.19522145,\n       23.7351858 , 25.7869005 , 25.30163805, 25.96993695, 30.69909555,\n       41.81751375, 51.1779288 , 48.8282466 , 47.1000402 , 52.51878   ,\n       46.85741025, 47.42780445, 46.27850415, 44.6694822 , 43.2477552 ,\n       43.49889705, 44.83123635, 40.8512499 , 43.4393061 , 38.01205185,\n       26.32323945, 24.3396327 , 24.7908399 , 24.8461749 , 25.727307  ,\n       26.5786398 , 28.94534835, 40.38301635, 47.36821095, 48.78993795,\n       46.69140015, 47.85347085, 45.89540235, 49.6242444 , 45.4527096 ,\n       43.17113535, 47.6704344 , 42.57520035, 42.30277365, 41.7792051 ,\n       41.57062785, 31.11625005, 25.6549431 , 24.7865814 , 24.34388865,\n       26.4509409 , 25.4037987 , 27.4470015 , 29.95417935, 39.5572218 ,\n       44.72056125, 48.47920005, 46.93828605, 46.2487074 , 44.66096775,\n       53.3360601 , 47.7427983 , 46.57221315, 49.50080145, 43.05194835,\n       43.49889705, 40.79591235, 34.63225635, 26.98727985, 25.2760998 ,\n       24.67590885, 24.60354495, 25.6889958 , 25.47190665, 28.0514484 ,\n       28.40049495, 31.98886515, 39.4678341 , 45.67405725, 42.9838404 ,\n       45.3079818 , 46.29978645, 46.99787955, 44.6822526 , 43.18390575,\n       44.51198655, 42.97107255, 42.25595055, 37.3565259 , 30.00100245,\n       26.08912395, 23.7479562 , 24.2928096 , 25.12711605, 26.0806095 ,\n       25.86352035, 25.3484637 , 26.6850564 , 28.20894405, 32.065485  ,\n       42.08142855, 44.9163681 , 45.4867623 , 43.6138281 , 44.9631912 ,\n       43.0817451 , 41.4216441 , 41.86859535, 38.8718991 , 35.8028415 ,\n       29.6476974 , 26.56161345, 24.55672185, 24.65462655, 25.15265685,\n       24.23747205, 25.59960555, 26.90640405, 24.42902295, 24.86745975,\n       26.7191091 , 29.73708765, 32.18041605, 38.31853125, 42.6433083 ,\n       42.1708188 , 46.2231666 , 43.67767755, 38.0418486 , 38.1397533 ,\n       34.90468305, 29.05176495, 26.23810515, 24.92279475, 24.66313845,\n       25.69325175, 23.84585835, 25.89757305, 26.433912  , 25.36123155,\n       25.2760998 , 26.16999975, 26.0337864 , 26.79998745, 27.9790845 ,\n       31.42272945, 37.12240785, 40.4596362 , 41.84305455, 40.48092105,\n       35.4921036 , 33.54254955, 28.1408361 , 27.1618044 , 25.3356933 ,\n       25.21225035, 25.71028065, 25.77838605, 24.21618975, 24.3736854 ,\n       26.57012535, 24.80360775, 24.6673944 , 24.96110595, 26.88086325,\n       27.0426174 , 26.54884305, 27.48531015, 30.87361755, 32.99769615,\n       34.1044293 , 32.92533225, 28.3068462 , 27.15328995, 25.67196945,\n       26.616951  , 24.7780695 , 27.30652965, 25.6421727 , 24.7652991 ,\n       26.20831095, 25.1441424 , 24.92705325, 26.4466824 , 28.29407835,\n       26.05081275, 25.6038615 , 26.4935055 , 27.7534809 , 26.64248925,\n       26.57438385, 25.8081828 , 26.70208275, 26.6254629 , 25.47190665,\n       24.6248298 , 24.5609778 , 26.3998593 , 24.3864558 , 26.91491595,\n       24.65036805, 24.53118105, 25.56555285, 27.0766701 ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ])"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcna_crops_flat_pad[0]*255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 510 2295 2295 6375 6885 6630 5610 7395 7650 5865 5355 3315 1785 2295\n",
      "  765 1275  765  765  510  765  255 1530  765  255  510  510 2040  765\n",
      "  510  255  255  510  765 1530 1275 1275 1530 2295 1020 1275 2040 1020\n",
      " 2295 1785 1785 2295 2040 2040 4335 1530 2040 1530 1275 1530 1275 1275\n",
      "  510  765  510  765  765  510  510  255  510]\n",
      "[ 1785  2040  3825  6630  7650  9435  6120 10965  4590  6120  5610  5355\n",
      "   765  1020  1275  1020  1275  1020   510   255   510   510   510   255\n",
      "  1020   255   255   510   510   255   255  1785  1275   765  1020   765\n",
      "   765  1275  1020   510   765  1020  1530  1275  2040  3315   510  1530\n",
      "  1785   765  2805  3060  2295  1275   255  1530   765   765  1275  1530\n",
      "   765   510   255   255   255]\n",
      "[ 3570  8925 13005 13515 10965  4080  2805  3825  2040  2295  2040  1785\n",
      "  1020  2040  2040  3060  2295  3315  1785  3825  3570  1785  2805  2550\n",
      "  1785  1785  1275  1785  1020  2040  2040  1785   765  1020   510  1020\n",
      "   255   765   255   765   510   510   255   255   255   255   255   765\n",
      "   255   255   255   255   255     0   255     0   510   255     0     0\n",
      "     0     0     0     0   255]\n",
      "[ 1530  5100 11730 12240  9435  7905  8925  3315  2805  2295  1785  1530\n",
      "   765  1020  1275   765  1020  1020   765   765   765   765   510   510\n",
      "   510   255   765  1530  1785  1530  2040  1020  1275   510  1020  2040\n",
      "  2295  2805  2295  1275   765  1785   765  2550  1785  2295  1275  1785\n",
      "  1275  1275  1785  1020  1275   510   255     0   510     0     0     0\n",
      "     0   255     0   255   510]\n",
      "[ 765 3825 4335 6120 5610 7650 6120 7395 7905 6120 3570 2805 1530 2040\n",
      " 2040 1275  255 1020  765  510  510    0 1275  510  510  510  510  255\n",
      "  510    0 1020  255  765  255  255  255 1530 2040 1275 1020  510 1785\n",
      "  765  510 1530 1785 1785 2550 2295 2805 2040 1785 3825 2295 1785 1530\n",
      " 1785 2040 2295  765  765  255  255  510  255]\n",
      "[3315 3315 6630 3315 2805 2805 3315 2295 2295 2550 1020 1020  765 1020\n",
      "  765  765  510  765  765 1020  765  255  510 1020  255  255  255  510\n",
      " 1275  510  255  765  255 1020  510  510 1785 2040 1275 1530 1530 1530\n",
      "  765 2550 2295 2295 1785 1530  765 2040 1275 1275 1530  255  765 1530\n",
      "    0  765  255  255  510    0    0  255  255]\n",
      "[ 1275  3060  9180 11730 14535 13515 10710  7905  6120  3570  2295  1785\n",
      "  1785  1785   510  1275   510   765  1275   765  1020   255   510   765\n",
      "   510  1275   255  1275   510   510   510  1785   765   765  1785  1020\n",
      "  1530  1020  1785  2295  1020  1275  2295  1785  2295  2040  2295  1785\n",
      "  1785  2295  2295  2295  2550  1275  1020  1785   255  1020   765     0\n",
      "     0     0     0     0   255]\n",
      "[ 2040  3315  9690 10965 11985  8925  6375 10710  4590  6120  4335  4590\n",
      "  2295  1785   510   765  1275   510   510   255   255   765   765   255\n",
      "   510   765  1020  1020   510     0  1020   765  1020   255   765   510\n",
      "  1020  1275   510  1785  2295  2040  1020  2295  2805  1275   765  3060\n",
      "  2805  2550  2550  4080  2805  2040  1530  1020  1530   255   510   765\n",
      "  1020   255   765   255   255]\n",
      "[2550 4080 5355 6120 7905 6120 5610 5865 3060 3315  510  765 1020 1275\n",
      "  510  510  765  765  765  255    0  510  510  510  510 1530 1275  765\n",
      "  765    0  765  510 1020 1275  765 1530 1275 1785 1785 1020 1530  765\n",
      " 1785 2550 2550 1275 1785 2295 2295 1530 1530 1275 1020 1275    0 1020\n",
      "  255  510    0  255  255  255    0    0  765]\n",
      "[  765  1020  3570 10200  7140 11985 11985  7905  3060  3825  2805   510\n",
      "   765  2295   510   255  1275   255   765  1020  1275  1275     0   255\n",
      "   255   255  1020  1020   255   510   510   255   765  1275  1530  2805\n",
      "  1785  2550  2550  2550  3060  2040  3315  3570  2040  1530   765  1785\n",
      "  2040  1275   510   255  1020  2295   765   255   765   255   255   255\n",
      "     0     0   510     0   255]\n",
      "[  510  4845  6630 10965  9435 13005  8670  6120  3825  2040  1275  1530\n",
      "  1275  1530  1020   510  1530   510   255   510   255   765   510  1020\n",
      "   255  1020   255   255   510   255   765   765  1020   765   765  1275\n",
      "  1275  1275  2040  3060  2295  4080  2805  3570  3315  2295  3060  1530\n",
      "  1530  1020   510   255   510   510   510   255   255   255   255     0\n",
      "   255     0     0     0   255]\n",
      "[  765  7905 11730 14025  8415  5355  3060  1530  1785   765  1785  2040\n",
      "   765   765  1020     0   510     0   765   765   255  1020   255   510\n",
      "   765   510  1020   765   765  2040  1785  2550  1785  1530  2040  1275\n",
      "  2550  1785  1785  2295  1020  1785  1530  1275   765     0   765   765\n",
      "   510   255   255     0   510   255   510   255     0     0     0     0\n",
      "   255     0     0     0   255]\n",
      "[ 2550  5610  9180 10710  8925  8160  8415  3315  3570  2295   510  1020\n",
      "  1020   510  1275  1020   765   510   255  1275  1275   765  1020   765\n",
      "   255   255   765   765  1020   765  1275   255  1785  2040  3825  1530\n",
      "  3060  2040  2040  2550  3570  2550  2550  2295  1530  1530  2295  1275\n",
      "  1785   765  2040   255   510   255     0     0   255     0   510   255\n",
      "     0     0     0     0   255]\n",
      "[ 1275  2805  6630 17085 29325 22695 14535  9180  5355  3825  1020  2550\n",
      "   765  1275  1530  1530  1275  1785  1275  2295  2040  1020  1530   765\n",
      "  1530  2805  3570  2550  1785  2040  2805  1275  1275  1275   255   255\n",
      "   765   765   510  1020   255   255   510   765  1020  1275   510   765\n",
      "  1785   510  1530  2040   255   510     0   510   510   510   255     0\n",
      "   255     0   255     0   255]\n",
      "[  255  1020  4845  7650  8670 12495  7905  7140  9180  9435  8670  8415\n",
      "  2805  3315  2805   765  1020  1530  1020  1275   765   255   510  1275\n",
      "  1020   510  1275   255   255   765  2040   765  1275  1020  1530  1530\n",
      "   510   765   255  1020  1275  1275   765  1530  1275  1530  2040  1275\n",
      "  1020  1530   765  2040  1275  1020  1785  1530  1530   765   765   765\n",
      "  1530   765   255   255   510]\n",
      "[2550 2805 6375 6120 7140 8670 6885 4335 3315 4590 1785 2040 1785 2805\n",
      " 4080  765 1530  255 1020  765 1020  765  510 1275  255 1020    0  255\n",
      " 1020  510 1530  255  255 1275  765  510  510 1275  765  510 1785  510\n",
      " 1530  510 1020 2805 1020 2550 2040 2040 2805  510 3060 2550 2295 2805\n",
      " 2040 1785 1275 1530 1275    0 1020  255  510]\n",
      "[ 1275  4335 12750 17340 21420 19125  7395  5100  4080  1785  1530  1020\n",
      "  1275   765   765  1530   510     0   765   765  1530  1020  1020   765\n",
      "  1020  1530   510  1020  1530  1785   765  2295   510  1530  1530   255\n",
      "  1785  1530  1785  1530  1530  2295  2040   765  1020  1785   765   510\n",
      "  1020   765   510  1020  1020   765   255  1020     0     0     0   255\n",
      "   255   255     0     0   255]\n",
      "[2040 3570 7905 8160 8160 5865 8670 7395 7650 6375 5100 1785 1530  765\n",
      " 1020  765  510  255  765  255 2295  510 1020 1275  765 1275 1020  765\n",
      "  510  255  510 1020  765  510  255  765  765  510  255 1020 1785 2040\n",
      " 1275 2040 1785 1275 1275 1275 1785 2040  510 1020 1785  255  510 2295\n",
      "  765  765  765  765  510 1020  255  255  765]\n",
      "[ 510 1530 2550 2040 3570 5865 7140 6120 6120 7395 6375 4335 4845 5865\n",
      " 2805 3060 3060 1785 1275 1530  255 1020  255 1020  255 1530  510  765\n",
      "  255 1020  510 1020  255 1530 1785 1275  765 1785 1530 1275 2295 1275\n",
      " 1530 1530 1275 1785  255 1275 1020 2040 1020 2295 1020 1020  255 1530\n",
      " 1785 1020  765    0  510    0  510  510  510]\n",
      "[ 5610 26775 28305 10200  4590  3570  1020  2295  2805  1785  1530  2805\n",
      "  2805  3060  5865  4590  5610  5610  4590  4845  4335  3315  2550  1530\n",
      "  2550   510  1020   765     0   255   510   255   255     0   255     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   255     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0   255]\n",
      "[ 1275  2295  6630  8670  6630 12240 10710  9690 12750  8670  4590  5100\n",
      "  4335  4335  3060   765  1275  2295   510  1020   255  1275  1275   255\n",
      "   510  1275     0  1530  1275   255  1785   765   255  1020  1785  1020\n",
      "   510  1275  2040  1530   510  1785  1530  2040  1275  1530  2040  2550\n",
      "  2805  2295  3825  2805  3315  3570  2550  1530  2295  1275  1785  1020\n",
      "  1275   765     0   510   765]\n",
      "[ 2040  4080  7650  6630  9690 11220  7905 10455  5355  3570  1020  1530\n",
      "  2295  1020   510  1020   510   510  1275   255  1530   255  1020  1020\n",
      "  1020   255     0   255   510   510   510   765   255   255  1530   510\n",
      "   765   765  1275  1275  1530  1785   765  1785  2040  2550   765  2295\n",
      "  2805  2295  2040  2295  1530   510  1275  1275   765  1275   510     0\n",
      "   255     0     0     0   255]\n",
      "[  765  5100 11730 14790 13005  5100  1785  2295  1530  1785  1785  1275\n",
      "   765   255  1275     0   510   510  1020     0   255  1020   255   765\n",
      "   255   765  1275   510   510   255  1530   510   255   255  1275   765\n",
      "   765  1785  2040   510  1275  1275  2040  2040  1275  1530   510  1530\n",
      "  1530   255  1530   510   765   765   510  1020   510   510   765   765\n",
      "     0     0     0     0   255]\n",
      "[ 1785  4080 13515 15300  7395  7140  6630  2550  1530  2550  1275  1785\n",
      "  1275   765  1530  1275   510  1275  1530   255   255  1530  1020  1785\n",
      "   765   510  1275  1020  1275   510  2040  1530   510   765  1020  1275\n",
      "  1020   765  1785  1785  3315  1020  1275  1530  4335  3060  2550  2040\n",
      "   510  2295   510   255   510   510   510   255   255   255   255   510\n",
      "   510   255     0     0   510]\n",
      "[ 2295 20655 28305 16575  6885  3060  2550  1785  1785  1020  1020  1530\n",
      "   765  1785  1275  1020  1785  1020  1020  3060  4080  2805  1530  1020\n",
      "  1785  1275  1785   510  1020   255   765   510   510   765   255   255\n",
      "     0   255   765   255   255     0   255   255     0   255   510   255\n",
      "   255   510     0   255   255     0     0     0   255     0     0     0\n",
      "     0   255     0     0   255]\n",
      "[3315 2805 5100 4845 6120 1785 2805 2550 1020  510 1530 1275  255    0\n",
      "  255 1020  255  510  765  510  765 1020  510  510 1275 1020  510  510\n",
      "  510 1020  765 1275 1020  255  765  255 1530  765  765 1530 1785  765\n",
      "  765  510  765  510  510 1530  765  255 1020  255  255  765  255  255\n",
      "    0    0    0    0  255    0    0    0  255]\n",
      "[ 1785  3315  9690 13260 12750 14280 10965  6885  5100  3060  2550  1530\n",
      "  1530  1275   765   255     0   510  2295  1275   510   510     0   255\n",
      "   255   765     0   255   510  1020  1275   510   510  1020   765  1530\n",
      "   765  1785  2040  2295   255  1785   510  1530  1785  1020   255   765\n",
      "   255   255  1020  1530   255   255   255  1020     0   255     0   510\n",
      "     0     0     0     0   255]\n",
      "[ 2550  5865  8925 17085 17595 16320 12240 12240  6120  3825  3315  3060\n",
      "  2040  2295  1020  1020  1020   510   510  1785  1530  1020   765  1785\n",
      "   765     0   255   255   510   510   765   765  1020   765   510   255\n",
      "   765   765   765  1020  2295  1275  1530  1020  1020   255  1275  2550\n",
      "  1530  1785  1020  2040  1785  2550  3825  1530  2040  1530  1020  1530\n",
      "  1785   765   765  1020   510]\n",
      "[  255  1020  2805  5355  7905  9435  9690 10965  7650  8160  8160  5100\n",
      "  2550  2295  2295  1530   255  1275   510  1020  1275  1275   510   510\n",
      "  1020   765   255  1530   255   765   255   765   255   765   510   765\n",
      "   510   765     0  2295   510  1530  1020   765  1020  2040  1785  2550\n",
      "  3315  2550  2040  3060  4590  3570  3315  1785  2040  2040  1275  1275\n",
      "   765     0   255   255   255]\n",
      "[ 1275  3825  9180 15555 10455  8415  9435  5100  1785  1785  1275  1020\n",
      "  2040  1275   510  1020   510   510     0   510     0   765   255   510\n",
      "   765   510   510   765   255   255   765   255   510   765  1020  1020\n",
      "   765   510   510  1020   510  1275  1020  1530   765  2550   255   510\n",
      "   765   765  1275   510  1275   510     0     0   255   255     0     0\n",
      "     0   255     0   255   255]\n",
      "[ 1785  3315  6375  7905  7395 11475 10200 10200  7650  5610  2295  1530\n",
      "  2295  3570  1020   255  2040   765   510  1275  1530   510   765   255\n",
      "   510   765  1020   765   765   510   255   765  1275  1020   510   765\n",
      "  1275   510  2550  1020  2040  1020  1530  1020  2040  2295  2295  2295\n",
      "  1530  3315  1785  2805  3570  2295  1530  2805  2295  1275  1020  1020\n",
      "  1020   510   255     0   510]\n",
      "[ 3060 11730 18360  9435  6375  3825  2040  3060  2040  2040  1785   765\n",
      "  1275   765  1530  2295  1530   255  2040  1785  1785  1785  1275  1275\n",
      "  1275  1785  1020  1530  1530  1530  1785  1020  1785     0   510  1530\n",
      "  1275  2040  1785  1275  1275  1020  2040  2040  1530  1530  1275     0\n",
      "  1530   765  1275     0   255   510   765   510     0   255   765   255\n",
      "   255   255   510   510   510]\n",
      "[ 5865 24735 34680 14280  7140  4590  1785  1530   765  1785  2040   765\n",
      "     0  1530   510  1020  1275  1275  1275  1020   765  3060  1785  2295\n",
      "  1020   765  1275  1275  1020  1275  1785  1020  2295  2040  2040  1785\n",
      "  1020  1530   510  1530   765  1785   510   510  1020   255   510   255\n",
      "   255     0   255   255   255   255     0   255   510   510   255   255\n",
      "     0     0     0     0   255]\n",
      "[ 3315 20145 25245 16065  6120   765  2550  1785  2805  1785  1020  1530\n",
      "  1020  1530  3060  2550  1530  3570  2295  1785  4845  3060  5100  2550\n",
      "  2040  3060  2295  2550  2805  1785  3315  1530  2040  2295   255   765\n",
      "  1020   765   765     0   510     0     0   255   255   510   510     0\n",
      "   255   255     0   255     0     0     0   255     0     0     0     0\n",
      "     0   255     0     0   255]\n",
      "[ 4335 11985 16575 12240  7140  5865  1275  2805  1020  1275  1530  1020\n",
      "     0  1020   255  1020     0     0   765     0   765  1020   765   765\n",
      "   765     0     0   510     0   255   765   255     0   255     0  1020\n",
      "   255   510  1020   765   510   510   510   510  1530     0   765  2295\n",
      "  1785  1530  1020  1020  1020  2040  1275  1020  1020  1020   765   765\n",
      "   255     0   765     0   255]\n",
      "[  765  1785  2040  4845  4335  7650 12240 14280  9435 13770 14535  7650\n",
      "  8160  3570  3060  2040  2295  1530  1530  1275  1020   510   765   765\n",
      "  2040   765   765   510  1530   255   510   255   765  1785   765  1020\n",
      "   510   510   765  1275   510  2040  1530  2040  1020  2550  1530  1785\n",
      "  2040  2550  3570  4080  2805  1785  2805  1020   255   255   510  1020\n",
      "   255  1020   765   510   255]\n",
      "[ 2805  8925 17850 21420 15555  8925  5610  2550  2295  1275  1020  1530\n",
      "  1020  1785   765   255   765   510   510   765   255  1275     0   510\n",
      "   765   255     0   510   510  1275   765   510     0  1275   255     0\n",
      "  1530   510   255  1530   765  1020  1275  1020  2550  1275  2040  2040\n",
      "  2295  2805  2805  2805  2805  2295  1275  1020  1020  1275  1530  1785\n",
      "  1020   510   510   510   510]\n",
      "[ 255 2040 3315 3825 6630 7140 6375 3825 2805 2550 2295 2040  765  510\n",
      " 1275  255  255  765    0 1275    0 1275  510  255  255  510  255  765\n",
      "  510  255 1020    0  510  765 1275 1530 1020  255  255 1530 1785  510\n",
      " 2295 1785 1275 2550 2040 1020  255  765 1275  765 1020  255 1275 1530\n",
      "  255  255    0    0    0    0  510    0  255]\n",
      "[  765  1275  4335  7140  6120 10455  5865  3570  2040  2805  1785   765\n",
      "  1020  1530  1275   510   765  1275  1020  1020  1020   510   765   255\n",
      "  2295   765  3060  2295  2805  2040  4080  6375  5610  5610  4590  4590\n",
      "  5610  3825  1785  1275  2040   510   765   510  1020   510   255     0\n",
      "     0   255     0     0     0   255   255   510   255   510   255   255\n",
      "   255     0   255     0   255]\n",
      "[ 5355 22695 27030 22950  9690  2550  3315  2040  2040  1275  2805  2550\n",
      "  2040  2550  2550  3825  3315  3570  1785  3315  2040  1020  1530  1020\n",
      "  1530  2040   765   765   255   765     0   765   510   510  1020   510\n",
      "   510   255   765   255   510   510   510     0     0   255   510     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0   255\n",
      "     0     0     0     0   255]\n",
      "[ 765 1785  765 2295 3315 3570 4845 7140 5865 4590 6120 7140 7650 4845\n",
      " 5865 5100 3060 3570 3825 2295 1020  510 1785 2040  765 1785 1020  765\n",
      "  765 1275 1020  255  510 1530 1530 1785 1530 1530 1530 2295 1785 1275\n",
      " 2805 2040 1530 3060 3570 1020 3570 3315 1530 1275 2295 2040 1530  765\n",
      "  765  765  510 1020    0  255  255    0  255]\n",
      "[  765 10455 28815 19635  6375  4845  2805  1275  1020  1275  1020   510\n",
      "  1020   765  1020  1785  1020   765  2040   255  1530  2550  1785  1275\n",
      "  2040  1020  1275   765  3060  2550  1020  1020  1020  1020  1275  2040\n",
      "  1020  1785   765   510   765   510     0  1020   765   765     0   510\n",
      "   765     0     0     0   510     0     0     0   510     0     0     0\n",
      "     0     0     0   255   255]\n",
      "[ 2550  4845 10710 11220 10200 10710 10710 12495  9945 10455  5610  5355\n",
      "   255  2805  1530  1530   765     0   765  1020  1275     0   765  1020\n",
      "   765     0   510  1275     0  1530  1020  1020  1020   510   765   765\n",
      "  1275   255  1275  2040  1275  4080  1785  2040  2805  1530  2550  4080\n",
      "  2550  1530  2295  2550  1530  2040  1020  1530  2040   510  1785   765\n",
      "     0   765   255   255   255]\n",
      "[ 4080 12495 22695 24735 15555 13260  8160  8670  4080  2295  1275  2295\n",
      "  1785  2550  2805  1275  1530  1275   765   255   255   765  1020   765\n",
      "  1275  1530  1020  1785   765  1785   765   510  1020   765  1785  1275\n",
      "  1785  2040  3060  3315  3570  2805  2550  3570  2550  4080  2295  4080\n",
      "  3570  2040  1785  2040  1275  1530  1275   765   255     0     0     0\n",
      "   255     0     0   255   255]\n",
      "[ 1785  5100 13260 18615 19635 15045  6885  5610  3060  3315   765  1530\n",
      "   765  1530  1020  1020  1020   765   765   510     0   510   765   765\n",
      "   510   765   510     0  1275  1530  1020   255   765  1020   510   765\n",
      "  1785  1530  1020  1530  1530  2040  1275  2805  1530  1530  2295  1785\n",
      "  1530  1020  1275  2295   255  1785   255   255  1020   765   510     0\n",
      "   765   510   510     0   510]\n",
      "[ 6375 17850 22185 23205 11985  5355  3060  1785  2550  1530   510  1530\n",
      "  2550  3315  3060  4080  4335  4590  5355  3315  4845  3570  4080  1275\n",
      "   510   255   255   255   765     0     0   255     0   510     0     0\n",
      "   510   255     0     0   255     0     0     0     0     0     0   255\n",
      "     0     0     0     0     0   255     0     0     0     0     0     0\n",
      "     0     0     0     0   255]\n",
      "[ 4335 23715 14280 15555 16320 11985  6885  4845  3060  3315  2295  1530\n",
      "  1785  2550  1785  3570  2550  2550  2805  2040  2040  2040  1530  1020\n",
      "  1530  1275  1275  1785  1530   765  1020  1785  1275  1785   765  2550\n",
      "   765  1275  1020  2040  1530  2805  1530   510   510  1020   510  1020\n",
      "   510  1020  1020   510  1020   255     0   255   255   255     0   255\n",
      "   255   255     0     0   255]\n",
      "[ 2550  6885 15810 16830 10200  9945  4845  3825  3825  3060  2805  2040\n",
      "  1275  1020   510   765  1020     0  1530   255   765  1020   255  1020\n",
      "  1275   765  1020   255  1530   510   765   510   765  1785  1020  1530\n",
      "  2040  1530  1530  1530  5100  3060  1530  1275  3060  1020  1785  1530\n",
      "  1785  1530   765  1275  1020  2550  1530  1275   510  1020  1785   765\n",
      "   765   510   255   510   255]\n",
      "[ 1275  3570 16320 20145 19125  7140  5865  2295  2550  2295  1020  1275\n",
      "   510   510   255  1020   765  1020     0   255   255   510   510   510\n",
      "  1020   255   255  1275   255   510   255  1020   255  1020  1020  1020\n",
      "  1275  1275  1020  1020  1785  1275  1275   765  2550  1530  2550   765\n",
      "  1785  1020  1020   765   765     0     0   510   765   510   765     0\n",
      "     0   765     0     0   510]\n",
      "[ 510 1275 5100 6630 5610 4335 3570 2805 1785 2295 1530  765 1020  255\n",
      "  255 1785  765 1020  510 1020  510  510  255  510  510  255  255 1530\n",
      "    0  255  255  255    0 1020    0  255  255  765  765 1785  765 1020\n",
      " 1275  765 1275 1785 1785 2040 1020 1275 1530 1530 2550 1275 2040 1275\n",
      " 1275  765 1020 1530  765  510  255    0  510]\n",
      "[ 510  765 1275 1020 1530 3315 1275 2040 1785 3825 5100 4335 3315 3570\n",
      " 3570 4590 1785 3315 2550 3315 2550 2040 2040 1275 1275  765  510 1020\n",
      " 1530 2295 1275 1275 1530 1530 2040 1275 3315 2550 2040 2805 1785 3315\n",
      " 3060 2040 1275 1530 3825 1785 1785 2550 3060 2550 1785 1530 1275  510\n",
      "  765    0  255  510  255    0    0    0  255]\n",
      "[ 1275  5610 11985 22695 19380 12240  7140  3060  2550  1530  1530  1020\n",
      "  1275   510  1020   510   510  1530  1275     0   510     0     0   510\n",
      "   510   765  1530   510   255     0  1020   510   765   765  1275   510\n",
      "   510   255  1020   765  1530   765  1275  1530  1530  2295  1275  2550\n",
      "  2295  4845  3060  3060  1785  2040  1530  1530   510  1785  1275   510\n",
      "   510   255     0     0   510]\n",
      "[ 5355  6375 11220  5610  4590  3315  2295  2295  1530  2295  1020   765\n",
      "     0  1020  1530   510     0   765   255   765   255   765     0   510\n",
      "   510   510   765   510  1020  1020  1020  1275  1275  1275  1275   510\n",
      "  1275  1020   255  2040  1275  2040  2550   765   765   510   255  1020\n",
      "   255  1530   255   255   255   255   765   765     0     0     0   255\n",
      "     0     0     0     0   255]\n",
      "[ 1530 11730 29580 21930 11730  5610  2295  2040  1020  2040  1275  1275\n",
      "  1530   510   510   510  1020  1020  1020  1275  1785  1020  1785  1275\n",
      "  2040  1275  1020  1020  1275  2550  2295   765  1785  1020  1530  1275\n",
      "  2295  2550  1275  2040  1275  1275  1530   765   765  1020  1020  1020\n",
      "  1530  1020   255   510   765   255   510   510   255     0   510     0\n",
      "   510     0     0     0   255]\n",
      "[ 1530  5100  8925 12495 16065 14790 10710  3570  3060  2805  1530   255\n",
      "  1020  1530  2295   765  2295   765  2295   255   765  1785  1020  1275\n",
      "  2040  1785  2040  2040  3315  2040  1530  2040  1275  1275  1530   510\n",
      "   765     0  1275  1785  1530  1530  1530  1530  1785  1785  1530  1785\n",
      "  1020  2040  2040   765   765   255   510   765   510  1020   765   510\n",
      "     0   255     0     0   510]\n",
      "[  255  1785 11730 20400 24480 19380  7395  3315  2040  2295  2295   765\n",
      "  2040   255   765   510  1530   255   765   510   255   255   765   255\n",
      "   765  1020   765   255   765   765   510   510   510  1275     0  1020\n",
      "   765  1020   510   765   510  1020  1275  1275  1530  2550  1785   765\n",
      "  1275  2550  2295  2805  1275  1020  1275  1530  2040  1785   765   765\n",
      "   510   255     0     0   510]\n",
      "[  510  1785  6375  9690 17595 15300 10200  7395  2805  1530  1785  2295\n",
      "  1530   510  1020   765   510   765   255   510  1275   765  1275   765\n",
      "   510   510  1020   255   510   765   255   765  1275  1275   510  1530\n",
      "   765   765  1020  1275   255  1275  1275  1785     0  1020   765  1275\n",
      "  2295  1275  1530  2040  1530  1020  1020  1020  1020   255  1020   510\n",
      "   255   510     0     0   255]\n",
      "[ 1785  2805  8160  8160  9690 10455 11220  8925  6120  6630  4335  4080\n",
      "  1275  2550  1785   510   255  1020   510     0  1020   765   765   765\n",
      "   510  1020     0   765   765   510   765   255   255   765   255   510\n",
      "   510   255   765   255   510     0   765  1785  1275  1275  1530  1020\n",
      "  1020  1275  2040  1275  1530  2040  2805  3060  2040  3570  2295  5100\n",
      "  2550  3315  2295   255   510]\n",
      "[1020 2550 2805 3060 3570 4335 4080 4080 2550 3315  510 2040  765  765\n",
      " 2040 1275 1785 1785  510 1275 2040 1530 1020 1785 1530 1020 2295 1020\n",
      " 2295  510  765    0 1275  765 2550 2805 1530 2040 2805 2295 1530 1530\n",
      " 2295 1530 1275 1530 1785  510 1275  510 1785 1785 1275 1530  510  255\n",
      "  765 1275    0  510  765  255  255  765  510]\n",
      "[ 2040 14790 11985 10455  5610  3570  2805   765  1020  1785  1020   255\n",
      "   510   510   510   765   765  1020   510   510  1275   765   510   255\n",
      "   255   255  1530   255   510   510  1275  1530   765   765  1275  1530\n",
      "   510  2805  1020  2040  1530  1020  1530  1785  1785  2040  1020  2550\n",
      "   510   765  1275  1275   765   510   765     0   765   510     0     0\n",
      "   255     0   255     0   255]\n",
      "[ 6120 23205 19635  8925  3570  2550  1020  1530     0  1020  1530   255\n",
      "   765  1020   765  1275  1020   510   510   255   255   765  1020   255\n",
      "  2040  1530  2295  1785  1530  1020  1275   510  1020   510  1530   510\n",
      "  1020   255  1275   255     0   510   765     0   765   255     0     0\n",
      "     0     0     0   255   510   255     0     0   510   255   255   510\n",
      "     0   255     0   510   255]\n",
      "[ 3315 17085 33150 21420  7395  3060  1785  1785  2040   765   510  1530\n",
      "   510   765   765  1020   765   765   510  1020   510  1275   510  1020\n",
      "   510   510   510   255  1020  1530  2295  1275  2295  2805  2040  1785\n",
      "  1275  2550  1530  2550  2040  2550  2040  3060  1020   765  1020   255\n",
      "  1275  1020   765   510   765   510   765     0   255     0     0     0\n",
      "     0     0     0     0   255]\n",
      "[ 1275  4080 14790 22440 20655 20145 10710 11475  4590  3825  1785  1530\n",
      "  1530  1785  1275     0   765  1020   510   765   255     0  1530  1020\n",
      "  1020   255   255   765  1020   765   765     0   510   765  1275   765\n",
      "  2295   765  1785  2040  1020  1020   765  2295  1530  1530  1275   510\n",
      "   510  1530   255  1785  2295  2295  1275  2040  1530   510  1530  1530\n",
      "   765   510   510     0   765]\n",
      "[ 3060  7650 11220  9180  4590  5100  2550  3315  1020  2550   765  2295\n",
      "   255  2805  3315  3570  5610  5100  3825  2550   765  1020   765  1020\n",
      "  1020  1020  1275   510   765  1020   765  1020  1530   510  1275  1020\n",
      "  1785  2295  1020  1275  1530  1275  1530  1020  1275  1020   255  1275\n",
      "   255  1275  1020  1275   765   255  1020   510   510   510  1020   510\n",
      "  1020   510   255   765   765]\n",
      "[ 2040  5355 16575 13515 13515 10710  7395  5610  4845  2550  3315  2805\n",
      "  1020   765  1275  1020  1785  2040   765   510  1020  1785  1530  1785\n",
      "  2040  1020  1530  2805  2550  2550  1785  2295  2040  1530  2295  3060\n",
      "  2040  1275  3060  1785  1785  2040  1275   510  1530   255     0     0\n",
      "   255     0     0     0   255     0     0     0   255     0   255     0\n",
      "   510     0     0   510   255]\n",
      "[ 4080 23715 22440 11985  5355  3060  2295  2295  1275  2805  1275  1020\n",
      "  1020  2295  1275   765  2295  2295  2295  1785  1785   255  2295  1020\n",
      "  1275   765  1020  2295  2040  1530  2550   510  2040   510   765  1530\n",
      "   765   255  1530   510     0     0     0  1275     0     0   510     0\n",
      "   255     0     0     0     0     0     0     0   255     0     0     0\n",
      "     0     0     0     0   255]\n",
      "[13770 16575 10200  4590  3825  1785  1530  1275   510  1020  1275   255\n",
      "   765   765   510   510   255   510  1020  1020   510     0  1020  1020\n",
      "  1020     0   255   765   765   510     0  1275   510   765  2295  1020\n",
      "   255  2550  2040  1785  2805  2295  2550  1530  1275   510  1530  1530\n",
      "  1785  1530  1020  1530     0   510   255   255     0   510     0     0\n",
      "     0     0     0     0   255]\n",
      "[ 6630 30345 29070 15300  8160  2550  4335  2550  1785     0   255  1275\n",
      "  1020   255  1530   765   255   510     0   765  1785   255   255  1020\n",
      "   765     0  1020   510   765   255   255   255  1530  1275  1020   510\n",
      "   510  1020   510  1020  1530  2040  1020  1275   510  1785  2550  2550\n",
      "  1785   765  1785  1530   765   510     0  1020   510   255   255     0\n",
      "     0   765   765   255   765]\n",
      "[16575 14025  6630  3315  1530  3315  1530   510  1530  1275  1020  1785\n",
      "  1020   765  1785  2295  1785  2295  1275  1020  1020  2040   765  1020\n",
      "  1020   765     0  1785  1785   765  1530  1020   510   765   510  1530\n",
      "  1020  1020   765  1530  2295   510  2040  1275  1530  1785  1785   510\n",
      "   765   510   510  1530     0     0     0     0     0     0     0     0\n",
      "   255     0     0     0   255]\n",
      "[ 510 4590 5865 6630 4080 2805 3570 3315 2550 2295 1530 1785 2040 1785\n",
      " 1020 1530 1275  765  510  510  510  765    0    0 1020    0    0  510\n",
      "  765  765    0 1020  255  510 1530  510  765  255  765  255  510 1530\n",
      " 1785 1530  765 1020 2295 1020  510 1020 1785  765 1020 1785 1020  765\n",
      " 1020  510    0    0  510  255 1275  510  255]\n",
      "[ 255  255 1530 1530 2295 3315 4590 4845 5355 8415 6375 3570 3825 2805\n",
      " 3825 2550 1785  765 1530 1530  765  765    0 1020  510  255    0  510\n",
      "  255  510  255    0 1275  510 1020  510  255  255  510  510  765  765\n",
      "  510  765 1020 1275 1020  765 2040 2805 1275 1275 3060 1785 1275 2550\n",
      " 2040 1530  255 1020 1275 1020  510  510  255]\n",
      "[ 765  510 2295 2805 7905 9435 8925 7395 6120 5355 1785 2295 2295 2295\n",
      "  765  255 1020  765  510 1020  510 1530  255  765  765  765 1020 1275\n",
      "  765    0  255  510 1275  510 1530  765 1275 1020  255 1020 1275 2040\n",
      " 1275 1530  510 2040 1530 1275 1275 1020 1020 1275 1020  765  510  765\n",
      " 1020    0    0  255  510  255    0    0  255]\n",
      "[ 2295 14280 24480 14025  6120  2805  2550  1785  1785  1530  1530  1275\n",
      "  1275  1530  1530  1785  1275   765  2805  3060  2040  3060  3060  5610\n",
      "  1785  3060  3825  2550  1275  1275  1785   510  1275     0   510     0\n",
      "   765   765     0   255     0   255   765     0     0     0   255     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0   255]\n",
      "[ 2550 10455 13005 15300 12240  6375  5355  4845  1020  1275  1530  1275\n",
      "   765     0   765  1020   510  1275   510  1020   510  1020   765   510\n",
      "     0   510   255   255   765   510   255  1020  1275  1020  1275  1275\n",
      "   510  1785  1275  1785  1275  2040  2040   510  3060  2040  2295  1275\n",
      "  1020   510   510  2040   255   765   510     0  1530   255   255   765\n",
      "   510  1020     0   765   510]\n",
      "[ 1530  3825  8925 15045 12240  9180  3060  4335  3060  3060  1020  1020\n",
      "  2040   765  2040  2040  1785  1785  1275   765  1020   765   765   255\n",
      "  1020   765     0   510   510   255   255  1275  1020   765   255  1020\n",
      "  1275   510   765   765   510   510   765  1275  1275  1785   765  1020\n",
      "  1530  2550  2040  1020  1275  1020  2805  2040  1530  1785  1785   255\n",
      "  1020   255   765   765   510]\n",
      "[ 7650 17340 17595 10965  7905  4335  4335  4845  3315  3315  4080  2040\n",
      "  2295   765  2040  2040  1275  1275   510  1020  1530  1020  1785  1020\n",
      "   765  2550  1530  2040  1530  1275  2805  3060  2805  2550  2040  1785\n",
      "  2550  1020  2295  1785  1275  1275  1275   255  1020     0     0     0\n",
      "     0     0   255   255   255   255     0   255   765   510   255   255\n",
      "   510     0     0   765   765]\n",
      "[ 255 1275 2805 5355 5610 5865 5865 3570 2295 1530 1530 1530  510  765\n",
      " 1020  765 1275  765  765 1020  765  510 1020  255  255 1530  255  765\n",
      " 1020 1020 1530 2040 2040 2295 1020 3315 1530  765 1530 1020 1785 1530\n",
      " 1275 2040 1785  765  510  765  765  510  255    0  255  510    0  510\n",
      "    0    0  255    0    0    0    0    0  255]\n",
      "[ 2295 19125 20400 13515  8415  4335  2040  1275  1785  1530  1530  2040\n",
      "   510  1275   510  1275  1530  1275  2295  1530  3825  2295  1020  1530\n",
      "  3060  2805  3060  2295   510  2805  1020  2295   510  1785   765   510\n",
      "   765   765   255     0   255     0   255  1275   510     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0   255     0\n",
      "   255     0     0     0   255]\n",
      "[1275 6885 7395 5610 7905 4590 4590 3315 2040 1275  765 1530 1020 1275\n",
      "  510  255 1275  510  765  765 1020  510  255    0 1275 2295 2040  765\n",
      " 1020 1785 2040 1530 1020 1530 1275 1530 1530  765  765  255  255  510\n",
      " 1530  765  255  255    0  255    0    0    0  510  510  255    0  255\n",
      "    0    0    0    0    0    0    0    0  510]\n",
      "[1275 3060 5610 2295 6120 5865 6885 5610 6120 6120 2550 2295 2550 1275\n",
      " 1020  255  255  510 1530 1530  510 1020  255  510  510 1020 1020  765\n",
      " 1020  765 1275  255  510 1530 1020  765 1785  765 2040 1785 2295 1530\n",
      " 1530 1275 1275 1785 1020 1785 1275    0 1020 1530  255  765  255  255\n",
      "  765  255    0  255  510  255    0    0  255]\n",
      "[ 765 1785 6375 8670 6375 4335 4080 2295  765 1275  765 1020  765  765\n",
      "  255  765 1275  765    0  765  255  510  255  765  255  510  255  510\n",
      "  255 1020 1020  255  255  765  510  765  510 1020  510 1020 1020  510\n",
      " 1020 1020  510  765 1530 2040 1275 2550 1275 1020 2805 1785 1785 1275\n",
      "  510 1275  255  510 1020  510    0  765  510]\n",
      "[ 3570  6885 10965 11475 12495  9690  6630  2550  1530  1785  1020  1275\n",
      "  1020   765  2040   510   765   765   765   255  1020   765   255  1020\n",
      "   765   510   765   765   510   510   510   510  1020   765  1275  1785\n",
      "  1020   510   510  1275  1275  2040   765  1530   765  2550  1020  1530\n",
      "  1020  1530  2805  1020  1020  1785  1785  1275   765  1530  1275   510\n",
      "  1020   510   510   510   255]\n",
      "[  765  2040  5610  8670 10455 12240  8670  5865  4845  4845  2295   765\n",
      "  1530  1530  1530   510   765  1020   765   765   765     0     0     0\n",
      "   510   510     0   255   765   255   765   765   510   765   765   510\n",
      "   510   765   765   765   765   510  1020   765  2040  2805  2295  2805\n",
      "  1785  2550  3060  2295  1785  3060  1530  2040  1530   765  1020  1785\n",
      "   255   765     0   510   255]\n",
      "[ 765 1275 3315 6885 6630 6885 4080 2805 2295 2040  510  765  255 1530\n",
      " 1020    0  765  255 1020    0  765  510  765  765    0  255 1020  765\n",
      "  255  255  255 1020  510  255    0 1275  510  765  510  510  510 1020\n",
      "  765  765  510 2295 1530  510 3060 1275 2550 1530 1020 2550 2040 2040\n",
      "  255  765 1275  510 1020    0  510    0  255]\n",
      "[ 510 1275 2805 3315 4080 5355 4080 4080 3570 2295 2550 1785 1785 1275\n",
      " 1275  510  255 1020  255  255  510  765  765  510  510 1020 1275  510\n",
      "  510  510  510  510  765 1020 1530 1020 2550 1785  765 1785 2295  510\n",
      " 2040 3825 2295 3825 1530 1530  765  510    0  255  255  510  255    0\n",
      "  255    0    0    0  255    0    0    0  255]\n",
      "[ 765 1020 3315 4335 5100 5610 8925 7650 9180 5355 3825 4590 2805 2550\n",
      " 1785  765 1020  510 1275 1275  510 1785  255  510  510 1020    0 1020\n",
      "  510  765  510  765 1020 1020  765  765 1275 1530  765  765  255 1275\n",
      " 1530 1530 1530 1785 2040 1020 1275 1530 2550 2040 3315 1530 2295 2805\n",
      " 1020 1275 1020 1275 1020  765  255    0  765]\n",
      "[ 765 1530 3315 4845 6375 7650 5865 5865 3570 3315 2040 3315  510 1020\n",
      " 1275 1275 1020  765  765  510  255  510 1530    0  765  255  255  255\n",
      "  510 1020  255 1275 1275  765  510 1020 1275  765    0  765  255 1275\n",
      " 1020 1275  765  510  765 2040 2550  510 2550 2040 1275 1530 1785 2040\n",
      "  765  765  510 1785 1530 2550  765 1020 1275]\n",
      "[  510  5355 10455 14025 10200  7140  3060  2040  2040  1020  1275   510\n",
      "  1020   510  1785   510   510   765  1275   255   765   255   255   255\n",
      "   255  1020   255   255   510   510   255   765   510  1275   510  1785\n",
      "   510   255  1530   765  1785  1020  2040  1785  1785  1275  2040   765\n",
      "  2040   765   765  1275   765   765   255   765   765  1020   255  1275\n",
      "   765     0   765     0   510]\n",
      "[1020 3060 5610 5355 4590 4845 4080 2805 1020 2040 1530 1020 1020    0\n",
      " 1020  510 1020  510  510  510  510  510  510 1275  765  510    0 1020\n",
      "  510  510  510 1275  255  510  255  765  765 1530 1530 1020 1275  765\n",
      " 2550 1020  510 2805 1275 1020 2295 2295 1785 1785 2040 3315 2295 3315\n",
      " 2295 2550  510  510  765 1785  510  255  765]\n",
      "[ 5865 17085 15555 15300 10710  7905  4590  1785   765  1530  1275  1275\n",
      "   510  1530   765  1530   255  1020     0   255  1020   765   510   255\n",
      "   765     0   510   510   255   255   765     0  1275   510  1020  1530\n",
      "  1020     0     0  1275  2040   765   765  1275   510  2040   765   765\n",
      "  2550  2295   765   255   765  1020   510  1020  1020   510   255   765\n",
      "     0   255     0   255   510]\n",
      "[ 2550 12495 13005 11730  7395  4845  1530  1530  2040  1020   765   765\n",
      "   765   510     0   765  1530   510   510     0   765   510   765   510\n",
      "   765   765   510  1020  1020  1785  1530  1530   510   765  1530  1275\n",
      "  1020  1785  2040  2295   765  2040  1020   765   765   510  1275   765\n",
      "  1020  1275   510   255   510   255     0     0  1275   255   255   255\n",
      "     0   255     0   510   510]\n",
      "[ 2805  5610 15045 15555 16320 12240 11220  6120  6375  2550  2040  2805\n",
      "  1530  2295  1020   255   765   765   510   255   765  1275   765   510\n",
      "   765  1785   510   765  1275   510   255  1275  1020   765  1020  1275\n",
      "  2295  3060  2295  1785  1275  2805  2550  2805  2805  3825  2805  2550\n",
      "  3825  2550  2040  2295  1785  1785  1275   510   510   510   765  1275\n",
      "   255   765   255   255   255]\n",
      "[ 510 3570 8415 9690 5610 3825 1785 3060 2550  510 1785  510  765    0\n",
      "  255  765 1275 1020  510  255  510    0    0  510  510  510 1020 1020\n",
      "  510    0  765  510  765  510  255  765  255  765  510  255    0 1020\n",
      " 1020  510 1020 1530 1275 1020 1020 2040 1275 1530 2295 1020 1530 1530\n",
      "  765  765 1530  765 1275  765    0  255  255]\n",
      "[ 255 1020 4845 6885 7395 4335 3060 2295 1020 2295  510 1020    0 1020\n",
      "  255 1020  510  510  765 3315 1275 3060 4335 5355 4845 2550 1530 1275\n",
      " 2550 2040 1785 1275 3570 1530 3060 2805 1275 1530 1020  765 1020  255\n",
      " 1020  510  510  765  510  510  510  510  255  255 1275  510 1020  510\n",
      " 1020  765    0  255    0    0    0    0  255]\n",
      "[ 6885 24735  9945  4845  2295  1530  2040  1275  1530  3060  1275  2805\n",
      "  5355  3825  2040  2295  2040  2805  1275  1020  2805  1530   510  2295\n",
      "  3825  2550  2295  2295  2295  2550  3060   765   765  1785   510  1275\n",
      "  1530  1020   765  1785   510  1530   255     0   510     0   255   255\n",
      "     0     0     0     0   255   255   255     0     0     0     0     0\n",
      "     0     0     0     0   255]\n",
      "[ 2805  3060  7140 10455 10455  6630  5610  4080  2295  2040   765   255\n",
      "  1530   510   255   510   765   510     0   510  1275  1020  1020     0\n",
      "  1020   255   255   255   255     0   255   765   765     0   510  1020\n",
      "   255   255   765   510   510   255  1785  1785   765  1530  1530  2550\n",
      "  1020   765   765  2295  1785  1020   765  3315  1020  1275  1785   255\n",
      "   765  1020   510  1020  1275]\n",
      "[ 8925 40800 37995  9180  3825  1530  1785  1530  1275   255  1020  1020\n",
      "  1785   765   510   765  1020   510   510  1020   765  1020   255   765\n",
      "   765     0   255   510  1530  1530  1020   255   765  1530  1530  1020\n",
      "   765  1785  1020  2295  1785  1275  1275   510  1530   510   255   510\n",
      "   765     0     0   255     0   255   510   510   255   255   255     0\n",
      "     0   255     0     0   510]\n",
      "[  765  4080  6375 12240  6885  4335  4335  2040  2295  1020  1530  2550\n",
      "  1020  1785   765  1530  1020   255  1530  1530   255  1020  1785  2040\n",
      "  1275  1020  1785  1020  3060  2040  1530  2295  1785   765   510  1275\n",
      "  1275  2295   765   765  1020  2295  1785  1785  1275   255  1530  1275\n",
      "   255   765   255     0     0     0   510   510     0   255   510   510\n",
      "     0     0     0   510   255]\n",
      "[ 2040  4335  7395 11985  9180  3315  1785  2295   510   510  2550     0\n",
      "  1275  1020  1275  1020   510   765   765   510   255  1020  1275   510\n",
      "   510  1020  1530  1530   255  2040  1275  1275  2040  3060  1785  2805\n",
      "  1275  2295  3060  1530  2295  1785  2295  1530  1785  1785   510   255\n",
      "   765   510   510   510  1530   765   255   510   255     0     0     0\n",
      "   510     0     0     0   255]\n",
      "[ 4845 21420 32640 13260  6885  3315  2295  2295  1530  1785   510  1275\n",
      "  1275  1275   510  1275  1275   765   765  2805  2040  2550  1530  2040\n",
      "  2805  2040  3570  3315  1785  1785  2805  1020  1275  1275  2295  1275\n",
      "   510  1275   510   510  1275   510   255  1020     0  1020   510     0\n",
      "     0   510   255   765   765   255   255   255   255   255     0     0\n",
      "     0     0     0     0   510]\n"
     ]
    }
   ],
   "source": [
    "#print(pcna_crops_flat_pad[0])\n",
    "for i in range(100):\n",
    "    image_id = i\n",
    "    new_arr = []\n",
    "    for i in pcna_crops_flat_pad[image_id]:\n",
    "        if i != 0: new_arr.append(i)\n",
    "    #plt.title(cell_labels[image_id])\n",
    "    #b, bins, patches = plt.hist(new_arr*255,bins=65,color='blue')\n",
    "    #plt.xlim([0,255])\n",
    "    hist, bins = np.histogram(new_arr*255,bins=65)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/26486\n",
      "100/26486\n",
      "200/26486\n",
      "300/26486\n",
      "400/26486\n",
      "500/26486\n",
      "600/26486\n",
      "700/26486\n",
      "800/26486\n",
      "900/26486\n",
      "1000/26486\n",
      "1100/26486\n",
      "1200/26486\n",
      "1300/26486\n",
      "1400/26486\n",
      "1500/26486\n",
      "1600/26486\n",
      "1700/26486\n",
      "1800/26486\n",
      "1900/26486\n",
      "2000/26486\n",
      "2100/26486\n",
      "2200/26486\n",
      "2300/26486\n",
      "2400/26486\n",
      "2500/26486\n",
      "2600/26486\n",
      "2700/26486\n",
      "2800/26486\n",
      "2900/26486\n",
      "3000/26486\n",
      "3100/26486\n",
      "3200/26486\n",
      "3300/26486\n",
      "3400/26486\n",
      "3500/26486\n",
      "3600/26486\n",
      "3700/26486\n",
      "3800/26486\n",
      "3900/26486\n",
      "4000/26486\n",
      "4100/26486\n",
      "4200/26486\n",
      "4300/26486\n",
      "4400/26486\n",
      "4500/26486\n",
      "4600/26486\n",
      "4700/26486\n",
      "4800/26486\n",
      "4900/26486\n",
      "5000/26486\n",
      "5100/26486\n",
      "5200/26486\n",
      "5300/26486\n",
      "5400/26486\n",
      "5500/26486\n",
      "5600/26486\n",
      "5700/26486\n",
      "5800/26486\n",
      "5900/26486\n",
      "6000/26486\n",
      "6100/26486\n",
      "6200/26486\n",
      "6300/26486\n",
      "6400/26486\n",
      "6500/26486\n",
      "6600/26486\n",
      "6700/26486\n",
      "6800/26486\n",
      "6900/26486\n",
      "7000/26486\n",
      "7100/26486\n",
      "7200/26486\n",
      "7300/26486\n",
      "7400/26486\n",
      "7500/26486\n",
      "7600/26486\n",
      "7700/26486\n",
      "7800/26486\n",
      "7900/26486\n",
      "8000/26486\n",
      "8100/26486\n",
      "8200/26486\n",
      "8300/26486\n",
      "8400/26486\n",
      "8500/26486\n",
      "8600/26486\n",
      "8700/26486\n",
      "8800/26486\n",
      "8900/26486\n",
      "9000/26486\n",
      "9100/26486\n",
      "9200/26486\n",
      "9300/26486\n",
      "9400/26486\n",
      "9500/26486\n",
      "9600/26486\n",
      "9700/26486\n",
      "9800/26486\n",
      "9900/26486\n",
      "10000/26486\n",
      "10100/26486\n",
      "10200/26486\n",
      "10300/26486\n",
      "10400/26486\n",
      "10500/26486\n",
      "10600/26486\n",
      "10700/26486\n",
      "10800/26486\n",
      "10900/26486\n",
      "11000/26486\n",
      "11100/26486\n",
      "11200/26486\n",
      "11300/26486\n",
      "11400/26486\n",
      "11500/26486\n",
      "11600/26486\n",
      "11700/26486\n",
      "11800/26486\n",
      "11900/26486\n",
      "12000/26486\n",
      "12100/26486\n",
      "12200/26486\n",
      "12300/26486\n",
      "12400/26486\n",
      "12500/26486\n",
      "12600/26486\n",
      "12700/26486\n",
      "12800/26486\n",
      "12900/26486\n",
      "13000/26486\n",
      "13100/26486\n",
      "13200/26486\n",
      "13300/26486\n",
      "13400/26486\n",
      "13500/26486\n",
      "13600/26486\n",
      "13700/26486\n",
      "13800/26486\n",
      "13900/26486\n",
      "14000/26486\n",
      "14100/26486\n",
      "14200/26486\n",
      "14300/26486\n",
      "14400/26486\n",
      "14500/26486\n",
      "14600/26486\n",
      "14700/26486\n",
      "14800/26486\n",
      "14900/26486\n",
      "15000/26486\n",
      "15100/26486\n",
      "15200/26486\n",
      "15300/26486\n",
      "15400/26486\n",
      "15500/26486\n",
      "15600/26486\n",
      "15700/26486\n",
      "15800/26486\n",
      "15900/26486\n",
      "16000/26486\n",
      "16100/26486\n",
      "16200/26486\n",
      "16300/26486\n",
      "16400/26486\n",
      "16500/26486\n",
      "16600/26486\n",
      "16700/26486\n",
      "16800/26486\n",
      "16900/26486\n",
      "17000/26486\n",
      "17100/26486\n",
      "17200/26486\n",
      "17300/26486\n",
      "17400/26486\n",
      "17500/26486\n",
      "17600/26486\n",
      "17700/26486\n",
      "17800/26486\n",
      "17900/26486\n",
      "18000/26486\n",
      "18100/26486\n",
      "18200/26486\n",
      "18300/26486\n",
      "18400/26486\n",
      "18500/26486\n",
      "18600/26486\n",
      "18700/26486\n",
      "18800/26486\n",
      "18900/26486\n",
      "19000/26486\n",
      "19100/26486\n",
      "19200/26486\n",
      "19300/26486\n",
      "19400/26486\n",
      "19500/26486\n",
      "19600/26486\n",
      "19700/26486\n",
      "19800/26486\n",
      "19900/26486\n",
      "20000/26486\n",
      "20100/26486\n",
      "20200/26486\n",
      "20300/26486\n",
      "20400/26486\n",
      "20500/26486\n",
      "20600/26486\n",
      "20700/26486\n",
      "20800/26486\n",
      "20900/26486\n",
      "21000/26486\n",
      "21100/26486\n",
      "21200/26486\n",
      "21300/26486\n",
      "21400/26486\n",
      "21500/26486\n",
      "21600/26486\n",
      "21700/26486\n",
      "21800/26486\n",
      "21900/26486\n",
      "22000/26486\n",
      "22100/26486\n",
      "22200/26486\n",
      "22300/26486\n",
      "22400/26486\n",
      "22500/26486\n",
      "22600/26486\n",
      "22700/26486\n",
      "22800/26486\n",
      "22900/26486\n",
      "23000/26486\n",
      "23100/26486\n",
      "23200/26486\n",
      "23300/26486\n",
      "23400/26486\n",
      "23500/26486\n",
      "23600/26486\n",
      "23700/26486\n",
      "23800/26486\n",
      "23900/26486\n",
      "24000/26486\n",
      "24100/26486\n",
      "24200/26486\n",
      "24300/26486\n",
      "24400/26486\n",
      "24500/26486\n",
      "24600/26486\n",
      "24700/26486\n",
      "24800/26486\n",
      "24900/26486\n",
      "25000/26486\n",
      "25100/26486\n",
      "25200/26486\n",
      "25300/26486\n",
      "25400/26486\n",
      "25500/26486\n",
      "25600/26486\n",
      "25700/26486\n",
      "25800/26486\n",
      "25900/26486\n",
      "26000/26486\n",
      "26100/26486\n",
      "26200/26486\n",
      "26300/26486\n",
      "26400/26486\n",
      "training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [26486, 126486]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [161]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m labels \u001B[38;5;241m=\u001B[39m cell_labels\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhistograms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m pcna_mod \u001B[38;5;241m=\u001B[39m LinearRegression()\u001B[38;5;241m.\u001B[39mfit(X_train,y_train)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPCNA accuracy:\u001B[39m\u001B[38;5;124m'\u001B[39m,np\u001B[38;5;241m.\u001B[39mcount_nonzero(y_test \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mround(pcna_mod\u001B[38;5;241m.\u001B[39mpredict(X_test))\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m))\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(X_test))\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2430\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2428\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2430\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2432\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   2433\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[0;32m   2434\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[0;32m   2435\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\sklearn\\utils\\validation.py:433\u001B[0m, in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[0;32m    415\u001B[0m \n\u001B[0;32m    416\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    429\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    432\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[1;32m--> 433\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    385\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 387\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    388\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    389\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    390\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [26486, 126486]"
     ]
    }
   ],
   "source": [
    "#Lets get the hisogram and the cell labels and make a linear regression on that\n",
    "\n",
    "histograms = []\n",
    "for i in range(len(pcna_crops_flat_pad)-100000):\n",
    "    if i % 100 == 0: print(str(i) + '/' + str(len(pcna_crops_flat_pad)-100000))\n",
    "    image_id = i\n",
    "    new_arr = []\n",
    "    for i in pcna_crops_flat_pad[image_id]:\n",
    "        if i != 0: new_arr.append(i)\n",
    "    #plt.title(cell_labels[image_id])\n",
    "    #b, bins, patches = plt.hist(new_arr*255,bins=65,color='blue')\n",
    "    #plt.xlim([0,255])\n",
    "    hist, bins = np.histogram(new_arr*255,bins=65)\n",
    "    histograms.append(hist)\n",
    "\n",
    "labels = cell_labels\n",
    "\n",
    "print('training')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCNA accuracy: 50.66062665156663\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(histograms, labels[:26486], test_size=0.1, random_state=42)\n",
    "pcna_mod = LinearRegression().fit(X_train,y_train)\n",
    "print('PCNA accuracy:',np.count_nonzero(y_test == np.round(pcna_mod.predict(X_test)).astype(int))*100/len(X_test))\n",
    "conf_pcna = pcna_mod.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Make a neural network\n",
    "\n",
    "#Let's get the X data which is the DAPI, Cyclin A2 etc. values\n",
    "#Since we use the same random state (42) it should be the same train and test models\n",
    "#I'll reduce the train size to have more testing data points\n",
    "#This is because we will use these test data points to make values predictions (DAPI etc.)\n",
    "#And these predictions will be part of the training for the neural network\n",
    "#This is because we are making this mega model that starts from PCNA to predict these values that will be used to predict the label\n",
    "#So the X data in this cell will be the value predictiosn (DAPI etc) that we'll generate from the testing data, then we'll split these values with the y being the cell_labels we just got from the cell above\n",
    "#Then we'll put these in a neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "cell_labels = get_cell_labels(df)\n",
    "\n",
    "def get_value_predictions(X_test, models, pcna_values):\n",
    "    all_preds = []\n",
    "    for model in models:\n",
    "        all_preds.append(model.predict(X_test))\n",
    "\n",
    "    value_predictions = pcna_values\n",
    "    for i in range(len(all_preds)):\n",
    "        value_predictions = np.dstack((value_predictions,all_preds[i]))\n",
    "\n",
    "    value_predictions = np.squeeze(value_predictions)\n",
    "    return value_predictions\n",
    "\n",
    "#really this is a split just to get the correct X_test\n",
    "#we're redoing an X_test right after this with the same random_state just to get the values_predictions as the X\n",
    "cell_labels = get_cell_labels(df)\n",
    "indices = np.arange(len(pcna_crops_flat_pad))\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(pcna_crops_flat_pad, cell_labels, indices, test_size=0.5, random_state=42, shuffle=False)\n",
    "pcna_values_del = np.delete(pcna_values, indices_train)\n",
    "values_predictions = get_value_predictions(X_test,[dapi_mod,cyclina2_mod,edu_mod,dna_mod],pcna_values_del)\n",
    "\n",
    "cell_labels = np.delete(cell_labels,indices_train) #dropping the training data points\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(values_predictions, cell_labels, test_size=0.5, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Let's make a mlp classifier that classifies using the real values of each DAPI and all\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#Then we'll decrease and observe by how much it changes\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m X \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdstack((\u001B[43mget_label_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m))\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m      5\u001B[0m y \u001B[38;5;241m=\u001B[39m get_cell_labels(df)\n\u001B[0;32m      6\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36mget_label_values\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m     10\u001B[0m     edu_values\u001B[38;5;241m.\u001B[39mappend(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124medu_values\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\n\u001B[0;32m     11\u001B[0m     dna_values\u001B[38;5;241m.\u001B[39mappend(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDNA_content\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\n\u001B[1;32m---> 12\u001B[0m     pcna_values\u001B[38;5;241m.\u001B[39mappend(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpcna_values\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     13\u001B[0m dapi_values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(dapi_values)\n\u001B[0;32m     14\u001B[0m cyclina2_values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(cyclina2_values)\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\pandas\\core\\series.py:958\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    955\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[0;32m    957\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[1;32m--> 958\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[0;32m    961\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[0;32m    962\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    963\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\pandas\\core\\series.py:1069\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[1;34m(self, label, takeable)\u001B[0m\n\u001B[0;32m   1066\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[0;32m   1068\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[1;32m-> 1069\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_get_values_for_loc(\u001B[38;5;28mself\u001B[39m, loc, label)\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\pandas\\core\\indexes\\range.py:385\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m    383\u001B[0m new_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(key)\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_range\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m(new_key)\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#Let's make a mlp classifier that classifies using the real values of each DAPI and all\n",
    "#Then we'll decrease and observe by how much it changes\n",
    "\n",
    "X = np.dstack((get_label_values(df))).squeeze()\n",
    "y = get_cell_labels(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8135920180889584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz200\\.conda\\envs\\celldev\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 4), random_state=1, max_iter=1000)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Accuracy function\n",
    "def class_pred_acc(model,values,gt):\n",
    "    preds = model.predict(values)\n",
    "    return accuracy_score(gt, preds)\n",
    "\n",
    "print(class_pred_acc(clf, X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Using pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.dstack((get_label_values(df))).squeeze()\n",
    "y = get_cell_labels(df)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42) #train-test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        \"g1\": 0,\n",
    "        \"s\": 0,\n",
    "        \"g2m\": 0\n",
    "    }\n",
    "\n",
    "    for i in obj:\n",
    "        if i == 0:\n",
    "            count_dict['g1'] += 1\n",
    "        elif i == 1:\n",
    "            count_dict['s'] += 1\n",
    "        elif i == 2:\n",
    "            count_dict['g2m'] += 1\n",
    "        else:\n",
    "            print(\"Check classes.\")\n",
    "\n",
    "    return count_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Class Distribution in Test Set')"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1800x504 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25,7))\n",
    "# Train\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_train)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Class Distribution in Train Set')\n",
    "# Validation\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_val)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[1]).set_title('Class Distribution in Val Set')\n",
    "# Test\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_test)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[2]).set_title('Class Distribution in Test Set')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class ClassifierDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "target_list = []\n",
    "for _, t in train_dataset:\n",
    "    target_list.append(t)\n",
    "\n",
    "target_list = torch.tensor(target_list)\n",
    "\n",
    "class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float)\n",
    "class_weights_all = class_weights[target_list]\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MulticlassClassification(\n",
      "  (layer_1): Linear(in_features=5, out_features=512, bias=True)\n",
      "  (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (layer_3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0007\n",
    "NUM_FEATURES = 5\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          sampler=weighted_sampler\n",
    "                          )\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)\n",
    "\n",
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(num_feature, 512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, num_class)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "090b2461e8984cbdb993544120b7e74c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.32649 | Val Loss: 0.44243 | Train Acc: 80.041| Val Acc: 79.267\n",
      "Epoch 002: | Train Loss: 0.30412 | Val Loss: 0.45078 | Train Acc: 80.713| Val Acc: 78.812\n",
      "Epoch 003: | Train Loss: 0.29974 | Val Loss: 0.45949 | Train Acc: 80.920| Val Acc: 77.824\n",
      "Epoch 004: | Train Loss: 0.29782 | Val Loss: 0.45539 | Train Acc: 80.930| Val Acc: 78.624\n",
      "Epoch 005: | Train Loss: 0.29549 | Val Loss: 0.46164 | Train Acc: 81.092| Val Acc: 78.229\n",
      "Epoch 006: | Train Loss: 0.29425 | Val Loss: 0.46541 | Train Acc: 81.206| Val Acc: 77.883\n",
      "Epoch 007: | Train Loss: 0.28849 | Val Loss: 0.43966 | Train Acc: 81.392| Val Acc: 78.506\n",
      "Epoch 008: | Train Loss: 0.28681 | Val Loss: 0.44731 | Train Acc: 81.362| Val Acc: 78.812\n",
      "Epoch 009: | Train Loss: 0.28734 | Val Loss: 0.43140 | Train Acc: 81.217| Val Acc: 79.158\n",
      "Epoch 010: | Train Loss: 0.28598 | Val Loss: 0.45622 | Train Acc: 81.365| Val Acc: 77.261\n",
      "Epoch 011: | Train Loss: 0.28455 | Val Loss: 0.45966 | Train Acc: 81.498| Val Acc: 77.982\n",
      "Epoch 012: | Train Loss: 0.27997 | Val Loss: 0.44481 | Train Acc: 81.896| Val Acc: 78.664\n",
      "Epoch 013: | Train Loss: 0.28462 | Val Loss: 0.47541 | Train Acc: 81.423| Val Acc: 77.221\n",
      "Epoch 014: | Train Loss: 0.28153 | Val Loss: 0.44484 | Train Acc: 81.581| Val Acc: 78.318\n",
      "Epoch 015: | Train Loss: 0.28175 | Val Loss: 0.48846 | Train Acc: 81.769| Val Acc: 76.322\n",
      "Epoch 016: | Train Loss: 0.27993 | Val Loss: 0.43529 | Train Acc: 81.924| Val Acc: 79.761\n",
      "Epoch 017: | Train Loss: 0.27751 | Val Loss: 0.44034 | Train Acc: 81.920| Val Acc: 79.217\n",
      "Epoch 018: | Train Loss: 0.27897 | Val Loss: 0.46620 | Train Acc: 81.603| Val Acc: 77.715\n",
      "Epoch 019: | Train Loss: 0.28130 | Val Loss: 0.45394 | Train Acc: 81.533| Val Acc: 78.545\n",
      "Epoch 020: | Train Loss: 0.27877 | Val Loss: 0.43121 | Train Acc: 82.058| Val Acc: 79.504\n",
      "Epoch 021: | Train Loss: 0.27567 | Val Loss: 0.50970 | Train Acc: 82.057| Val Acc: 76.174\n",
      "Epoch 022: | Train Loss: 0.27763 | Val Loss: 0.46477 | Train Acc: 81.936| Val Acc: 77.784\n",
      "Epoch 023: | Train Loss: 0.27672 | Val Loss: 0.46410 | Train Acc: 81.936| Val Acc: 77.458\n",
      "Epoch 024: | Train Loss: 0.27644 | Val Loss: 0.47998 | Train Acc: 81.979| Val Acc: 77.330\n",
      "Epoch 025: | Train Loss: 0.27433 | Val Loss: 0.45465 | Train Acc: 81.949| Val Acc: 77.972\n",
      "Epoch 026: | Train Loss: 0.27281 | Val Loss: 0.43241 | Train Acc: 82.249| Val Acc: 79.415\n",
      "Epoch 027: | Train Loss: 0.27913 | Val Loss: 0.42837 | Train Acc: 81.750| Val Acc: 79.454\n",
      "Epoch 028: | Train Loss: 0.27464 | Val Loss: 0.42348 | Train Acc: 82.069| Val Acc: 79.751\n",
      "Epoch 029: | Train Loss: 0.27697 | Val Loss: 0.43986 | Train Acc: 81.689| Val Acc: 78.348\n",
      "Epoch 030: | Train Loss: 0.27545 | Val Loss: 0.45775 | Train Acc: 81.915| Val Acc: 78.170\n",
      "Epoch 031: | Train Loss: 0.27623 | Val Loss: 0.47127 | Train Acc: 81.871| Val Acc: 77.626\n",
      "Epoch 032: | Train Loss: 0.27486 | Val Loss: 0.46363 | Train Acc: 81.951| Val Acc: 77.705\n",
      "Epoch 033: | Train Loss: 0.27004 | Val Loss: 0.43687 | Train Acc: 82.183| Val Acc: 78.792\n",
      "Epoch 034: | Train Loss: 0.27667 | Val Loss: 0.45253 | Train Acc: 81.858| Val Acc: 78.150\n",
      "Epoch 035: | Train Loss: 0.27558 | Val Loss: 0.45034 | Train Acc: 81.910| Val Acc: 78.694\n",
      "Epoch 036: | Train Loss: 0.27394 | Val Loss: 0.45080 | Train Acc: 81.893| Val Acc: 78.239\n",
      "Epoch 037: | Train Loss: 0.27480 | Val Loss: 0.46277 | Train Acc: 81.961| Val Acc: 77.913\n",
      "Epoch 038: | Train Loss: 0.27510 | Val Loss: 0.43298 | Train Acc: 81.961| Val Acc: 79.079\n",
      "Epoch 039: | Train Loss: 0.27545 | Val Loss: 0.45146 | Train Acc: 81.735| Val Acc: 78.476\n",
      "Epoch 040: | Train Loss: 0.27233 | Val Loss: 0.44763 | Train Acc: 82.115| Val Acc: 78.041\n",
      "Epoch 041: | Train Loss: 0.27142 | Val Loss: 0.43771 | Train Acc: 82.216| Val Acc: 78.921\n",
      "Epoch 042: | Train Loss: 0.27702 | Val Loss: 0.44651 | Train Acc: 81.717| Val Acc: 78.941\n",
      "Epoch 043: | Train Loss: 0.27238 | Val Loss: 0.48630 | Train Acc: 82.106| Val Acc: 76.292\n",
      "Epoch 044: | Train Loss: 0.27206 | Val Loss: 0.45979 | Train Acc: 82.146| Val Acc: 78.308\n",
      "Epoch 045: | Train Loss: 0.27311 | Val Loss: 0.42856 | Train Acc: 82.285| Val Acc: 79.454\n",
      "Epoch 046: | Train Loss: 0.27136 | Val Loss: 0.44002 | Train Acc: 82.041| Val Acc: 78.960\n",
      "Epoch 047: | Train Loss: 0.27304 | Val Loss: 0.44195 | Train Acc: 82.226| Val Acc: 78.921\n",
      "Epoch 048: | Train Loss: 0.27075 | Val Loss: 0.45856 | Train Acc: 82.086| Val Acc: 77.863\n",
      "Epoch 049: | Train Loss: 0.27274 | Val Loss: 0.43794 | Train Acc: 82.072| Val Acc: 79.118\n",
      "Epoch 050: | Train Loss: 0.27428 | Val Loss: 0.45056 | Train Acc: 82.032| Val Acc: 78.031\n",
      "Epoch 051: | Train Loss: 0.27306 | Val Loss: 0.44544 | Train Acc: 82.093| Val Acc: 78.812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m multi_acc(y_train_pred, y_train_batch)\n\u001B[0;32m     19\u001B[0m train_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 20\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m train_epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m train_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     23\u001B[0m train_epoch_acc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m train_acc\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[1;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[1;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\optim\\adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    138\u001B[0m             \u001B[38;5;66;03m# record the step after step update\u001B[39;00m\n\u001B[0;32m    139\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 141\u001B[0m     \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m           \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m           \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m           \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m           \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m           \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\.conda\\envs\\celldev\\lib\\site-packages\\torch\\optim\\_functional.py:98\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m     97\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m---> 98\u001B[0m \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad:\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001B[39;00m\n\u001B[0;32m    101\u001B[0m     torch\u001B[38;5;241m.\u001B[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001B[38;5;241m=\u001B[39mmax_exp_avg_sqs[i])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "\n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_train_pred = model(X_train_batch)\n",
    "\n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "\n",
    "\n",
    "    # VALIDATION\n",
    "    with torch.no_grad():\n",
    "\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "            y_val_pred = model(X_val_batch)\n",
    "\n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n              predictor='auto', random_state=0, reg_alpha=0, ...)",
      "text/html": "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n              importance_type=None, interaction_constraints=&#x27;&#x27;,\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;,\n              predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n              importance_type=None, interaction_constraints=&#x27;&#x27;,\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;,\n              predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0, ...)</pre></div></div></div></div></div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trying xgboot classifier\n",
    "import xgboost\n",
    "\n",
    "X = np.dstack((get_label_values(df))).squeeze()\n",
    "y = get_cell_labels(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42, shuffle=True)\n",
    "\n",
    "xg = xgboost.XGBClassifier()\n",
    "xg.fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8284584980237154\n"
     ]
    }
   ],
   "source": [
    "#Accuracy function\n",
    "from sklearn.metrics import accuracy_score\n",
    "def class_pred_acc(model,values,gt):\n",
    "    preds = model.predict(values)\n",
    "    return accuracy_score(gt, preds)\n",
    "\n",
    "print(class_pred_acc(xg,X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125221 1265\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Look into if there's more features we can collect on these cells\n",
    "#Could be good for an RFC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Could also make use of the PCNA images with the other data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26701209 0.07297885 0.21502775 0.08167543 1.07590322]\n",
      " [0.31337756 0.0936574  0.2561175  0.04560653 1.33358816]\n",
      " [0.26285994 0.10235551 0.21741169 0.04931917 1.49632523]\n",
      " ...\n",
      " [0.28314161 0.08428298 0.20223175 0.0636438  1.22602824]\n",
      " [0.26230655 0.10990231 0.22570132 0.08299008 1.61579281]\n",
      " [0.29674985 0.12230165 0.21580757 0.08387676 1.80925236]]\n"
     ]
    }
   ],
   "source": [
    "print(values_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "#Let's train the neural network now\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=10000)\n",
    "clf = clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6489153121244703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Accuracy function\n",
    "def class_pred_acc(model,values,gt):\n",
    "    preds = model.predict(values)\n",
    "    return accuracy_score(gt, preds)\n",
    "\n",
    "print(class_pred_acc(clf, X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6467965340585669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "sgd = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd = sgd.fit(X_train, y_train)\n",
    "print(class_pred_acc(sgd,X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "cell_labels = get_cell_labels(df)\n",
    "indices = np.arange(len(pcna_crops_flat_pad))\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(pcna_crops_flat_pad, cell_labels, indices, test_size=0.5, random_state=42, shuffle=False)\n",
    "pcna_values_del = np.delete(pcna_values, indices_train)\n",
    "values_predictions = get_value_predictions(X_test,[dapi_mod,cyclina2_mod,edu_mod,dna_mod],pcna_values_del)\n",
    "\n",
    "cell_labels = np.delete(cell_labels,indices_train) #dropping the training data points\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(values_predictions, cell_labels, test_size=0.5, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4366807393703651"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "X_features = rbf_feature.fit_transform(values_predictions)\n",
    "clf = SGDClassifier(max_iter=1000)\n",
    "clf.fit(X_features, y)\n",
    "SGDClassifier(max_iter=1000)\n",
    "clf.score(X_features, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "def get_elbow(curve):\n",
    "    # source:https://stackoverflow.com/questions/2018178/finding-the-best-trade-off-point-on-a-curve\n",
    "    nPoints = len(curve)\n",
    "    allCoord = np.vstack((range(nPoints), curve)).T\n",
    "    np.array([range(nPoints), curve])\n",
    "    firstPoint = allCoord[0]\n",
    "    lineVec = allCoord[-1] - allCoord[0]\n",
    "    lineVecNorm = lineVec / np.sqrt(np.sum(lineVec ** 2))\n",
    "    vecFromFirst = allCoord - firstPoint\n",
    "    scalarProduct = np.sum(vecFromFirst * np.matlib.repmat(lineVecNorm, nPoints, 1), axis=1)\n",
    "    vecFromFirstParallel = np.outer(scalarProduct, lineVecNorm)\n",
    "    vecToLine = vecFromFirst - vecFromFirstParallel\n",
    "    distToLine = np.sqrt(np.sum(vecToLine ** 2, axis=1))\n",
    "    idxOfBestPoint = np.argmax(distToLine)\n",
    "    return idxOfBestPoint\n",
    "\n",
    "def get_avg_split_arr(arr, num_splits):\n",
    "    array_split = np.array_split(arr, num_splits)\n",
    "    averages = [mean(array) for array in array_split]\n",
    "    return averages\n",
    "\n",
    "def get_EdU_threshold(edu_nums):\n",
    "    #edu_nums = np.sort(np.array(df.loc[:, [column_name]]).flatten())  # extract EdU values and put them in one array shape num_of_cells\n",
    "    # Get the list of EdU values\n",
    "    # Make a split from 3 to the total number of cells\n",
    "    # Get the averages at that split\n",
    "    # Get the elbow in those averages\n",
    "    # Store that elbow in a list\n",
    "    # Average out the list of elbow and return that value\n",
    "    elbows_y = []\n",
    "    start = round(0.73*len(edu_nums)) #we don't want to average out all of the cells because that would create a much a lower value so we only keep the top quarter\n",
    "    for i in range(start, len(edu_nums) + 1):\n",
    "        avg_split_arr = get_avg_split_arr(edu_nums, i)\n",
    "        elbow_x = get_elbow(avg_split_arr)\n",
    "        elbows_y.append(edu_nums[elbow_x])\n",
    "    return mean(elbows_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "\n",
    "def apply_rules(values_predictions):\n",
    "    #want to separate out the dapi values from the cyclin a2 values and the others etc.\n",
    "    dapi_values = values_predictions[:,0]\n",
    "    cyclina2_values = values_predictions[:,1]\n",
    "    edu_values = values_predictions[:,2]\n",
    "    dna_values = values_predictions[:,3]\n",
    "\n",
    "    edu_thresh = get_EdU_threshold(edu_values[:250])\n",
    "    cyclina2_thresh = get_EdU_threshold(cyclina2_values[:250])\n",
    "\n",
    "    preds = []\n",
    "    for i in range(len(values_predictions)):\n",
    "        if edu_values[i] >= edu_thresh: preds.append(1) #s\n",
    "        elif cyclina2_values[i] < cyclina2_thresh and edu_values[i] < edu_thresh and dna_values[i] < 1.5: preds.append(0) #g1\n",
    "        elif dna_values[i] > 1.5 and cyclina2_values[i] > cyclina2_thresh and edu_values[i] < edu_thresh: preds.append(2) #g2m\n",
    "\n",
    "    return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "cell_labels = get_cell_labels(df)\n",
    "indices = np.arange(len(pcna_crops_flat_pad))\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(pcna_crops_flat_pad, cell_labels, indices, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "values_predictions = get_value_predictions(X_test,[dapi_mod,cyclina2_mod,edu_mod,dna_mod])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    X  y\n",
      "0   [0.07297884556833448, 0.21502774912679135, 0.0...  0\n",
      "1   [0.0936574029812724, 0.25611750030191666, 0.04...  0\n",
      "2   [0.10235550715260495, 0.21741169459594326, 0.0...  0\n",
      "3   [0.0746306168712752, 0.21827037606024718, 0.09...  0\n",
      "4   [0.05883775160158323, 0.1610087322680655, 0.01...  0\n",
      "5   [0.19211636390264408, 0.3190461898414219, 0.11...  0\n",
      "6   [0.07468121534336553, 0.2047367302888031, 0.09...  0\n",
      "7   [0.07348525783072753, 0.2257221851233472, 0.07...  0\n",
      "8   [0.07829013783588076, 0.2065336391920153, 0.06...  0\n",
      "9   [0.06554901224195551, 0.1972760959235117, 0.06...  0\n",
      "10  [0.0748519808713347, 0.1944836572266424, 0.057...  0\n",
      "11  [0.07428322736241541, 0.2054121124224478, 0.03...  0\n",
      "12  [0.07337421166564737, 0.19928498621839658, 0.0...  0\n",
      "13  [0.05024048345919645, 0.2070022426461826, 0.06...  0\n",
      "14  [0.07491501975437462, 0.1949924865625282, 0.07...  0\n",
      "15  [0.06642725550620043, 0.18841266345388183, 0.0...  0\n",
      "16  [0.07551358044857376, 0.16506382976472303, 0.0...  0\n",
      "17  [0.07175549309303034, 0.20173467897949338, 0.0...  0\n",
      "18  [0.07553251766806844, 0.19588673161384756, 0.0...  0\n",
      "19  [0.07241770814379536, 0.17740654392086566, 0.0...  0\n"
     ]
    }
   ],
   "source": [
    "X = values_predictions\n",
    "y = cell_labels\n",
    "\n",
    "df_test = pd.DataFrame(list(zip(X,y)),columns=['X','y'])\n",
    "print(df_test[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.68790588065984\n"
     ]
    }
   ],
   "source": [
    "#Result from applying the rules\n",
    "#This may be due to the thresholds not being representative as we usually calculate them on one cell image not that man\n",
    "\n",
    "predictions_rules = apply_rules(X)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(predictions_rules)):\n",
    "    if predictions_rules[i] == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct*100/len(predictions_rules))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "#Lets try randomforest\n",
    "#why am I not including PCNA in this??? thats the real next step hahaha\n",
    "\n",
    "#lets make a dataframe\n",
    "\n",
    "df_rfc = pd.DataFrame(list(zip(values_predictions[:,0],values_predictions[:,1],values_predictions[:,2],values_predictions[:,3],cell_labels)),columns=['dapi','cyclina2','edu','dna','labels'])\n",
    "\n",
    "X = df_rfc.iloc[:, 0:4].values\n",
    "y = df_rfc.iloc[:, 4].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "regressor = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "#print(confusion_matrix(y_test,y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.904577436951534\n"
     ]
    }
   ],
   "source": [
    "#print(classification_report(y_test,y_pred))\n",
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct*100/len(y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#we also need to compare then with the ground truth values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#need to print a bunch of them with their predictions and their real value next to them\n",
    "#especially for the edu_mod one that seems terribly wrong"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#need to plot the predictions and the rest by showing how much they are off by"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Get train and test. Done\n",
    "\n",
    "#Time the first 1000 to see how long it will take us for the 138k. Not useful right now\n",
    "\n",
    "#could also augment with rotations, although they might be ignored here since the crop is flattened. Need to research this.\n",
    "\n",
    "#also need to make these predictions on other values, lets put all of this into a function. Done\n",
    "\n",
    "#instead of applying the rules, we could make a tiny NN that takes these 4 values and predicts the label\n",
    "\n",
    "#So to get the new data and only get the segmentations not the entire crop with the background I'll have to go back to Richmond and that will take another 4 hours\n",
    "#Anyway, for now I'm seeing if this is able to guess the cell label accordingly\n",
    "#Could try once with applying the rules\n",
    "\n",
    "#will try dropping some model predictions just to see if it performs better without them\n",
    "#to be faire a neural network would learn to ignore the useless ones\n",
    "\n",
    "#might be better to get confidence labels rather than hard classification\n",
    "#it would be cool to compare that with the confidence with the threshold (how far it is from the threshold) for the training data rules classification\n",
    "\n",
    "#will try applying the rules on those predictions after, could make a simple function now that we have the 'get_value_predictions' function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Later on we will want to just have the segmentations of the cells from the mask and not have the background, as in the background should just be zeros, that would probably even make this model more accurate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}